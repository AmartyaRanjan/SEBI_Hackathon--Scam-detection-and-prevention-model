{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13005109,"sourceType":"datasetVersion","datasetId":8233187},{"sourceId":13005413,"sourceType":"datasetVersion","datasetId":8233394},{"sourceId":13005512,"sourceType":"datasetVersion","datasetId":8233460},{"sourceId":13009858,"sourceType":"datasetVersion","datasetId":8236577},{"sourceId":13010090,"sourceType":"datasetVersion","datasetId":8236723},{"sourceId":13059310,"sourceType":"datasetVersion","datasetId":8269878},{"sourceId":13061316,"sourceType":"datasetVersion","datasetId":8271112},{"sourceId":13115375,"sourceType":"datasetVersion","datasetId":8308124},{"sourceId":13117536,"sourceType":"datasetVersion","datasetId":8309624},{"sourceId":13117573,"sourceType":"datasetVersion","datasetId":8309645}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-whois pyzbar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:58:42.557603Z","iopub.execute_input":"2025-09-20T17:58:42.558456Z","iopub.status.idle":"2025-09-20T17:58:48.525676Z","shell.execute_reply.started":"2025-09-20T17:58:42.558425Z","shell.execute_reply":"2025-09-20T17:58:48.524613Z"}},"outputs":[{"name":"stdout","text":"Collecting python-whois\n  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\nCollecting pyzbar\n  Downloading pyzbar-0.1.9-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from python-whois) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->python-whois) (1.17.0)\nDownloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzbar-0.1.9-py2.py3-none-any.whl (32 kB)\nInstalling collected packages: pyzbar, python-whois\nSuccessfully installed python-whois-0.9.5 pyzbar-0.1.9\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom io import StringIO\n\ndef get_bulk_deal_client_frequency():\n    \"\"\"\n    Fetches daily bulk deal data from the BSE and calculates the frequency\n    of each client name involved in the trades.\n    \"\"\"\n    print(\"Fetching daily bulk deal data from BSE...\")\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        all_tables = pd.read_html(StringIO(response.text))\n        \n        deals_df = None\n        for table in all_tables:\n            if 'Client Name' in table.columns:\n                deals_df = table\n                break\n        \n        if deals_df is None or deals_df.empty:\n            print(\"❌ Could not find or parse the bulk deals table today.\")\n            return None\n\n        # Use value_counts() to get the frequency of each name\n        name_counts = deals_df['Client Name'].value_counts()\n        print(\"✅ Successfully calculated client name frequencies.\")\n        return name_counts\n\n    except Exception as e:\n        print(f\"❌ An error occurred: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    client_frequencies = get_bulk_deal_client_frequency()\n    \n    if client_frequencies is not None and not client_frequencies.empty:\n        print(\"\\n\" + \"=\"*50)\n        print(\"Frequency of Client Names in Today's Bulk Deals\")\n        print(\"=\"*50)\n        # The result from value_counts() is a pandas Series\n        for name, count in client_frequencies.items():\n            print(f\"- {name}: {count} trade(s)\")\n    else:\n        print(\"\\nNo bulk deals to analyze today.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:25.057525Z","iopub.execute_input":"2025-09-10T04:31:25.057784Z","iopub.status.idle":"2025-09-10T04:31:28.352538Z","shell.execute_reply.started":"2025-09-10T04:31:25.057760Z","shell.execute_reply":"2025-09-10T04:31:28.351750Z"}},"outputs":[{"name":"stdout","text":"Fetching daily bulk deal data from BSE...\n✅ Successfully calculated client name frequencies.\n\n==================================================\nFrequency of Client Names in Today's Bulk Deals\n==================================================\n- IRAGE BROKING SERVICES LLP: 6 trade(s)\n- NEO APEX VENTURE LLP: 5 trade(s)\n- MANSI SHARE AND STOCK BROKING PRIVATE LIMITED: 4 trade(s)\n- NEO APEX SHARE BROKING SERVICES LLP: 4 trade(s)\n- PRAS INVESTMENT PRIVATE LIMITED: 3 trade(s)\n- PARNIT VENTURES PRIVATE LIMITED: 2 trade(s)\n- B N RATHI SECURITIES LIMITED: 2 trade(s)\n- AKSHAY PALIWAL: 2 trade(s)\n- SYLPH TECHNOLOGIES LIMITED: 2 trade(s)\n- SAROJDEVI P GUPTA: 2 trade(s)\n- BHATIA NIKHIL MURLIDHAR: 2 trade(s)\n- ISHAAN TRADEFIN LLP: 2 trade(s)\n- F3 ADVISORS PRIVATE LIMITED: 2 trade(s)\n- QE SECURITIES LLP: 2 trade(s)\n- VARANGA PROPERTIES PRIVATE LIMITED: 2 trade(s)\n- PRASHANT GUPTA: 2 trade(s)\n- VIKRAMKUMAR KARANRAJ SAKARIA HUF: 2 trade(s)\n- CHAUHAN NAGJIBHAI CHANDUBHAI: 2 trade(s)\n- KAMLESH NAVINCHANDRA SHAH: 2 trade(s)\n- SHAILESH DHAMELIYA: 2 trade(s)\n- SHARE INDIA SECURITIES LIMITED: 2 trade(s)\n- NEOMILE CORPORATE ADVISORY PRIVATE LIMITED: 2 trade(s)\n- ALACRITY SECURITIES LIMITED: 2 trade(s)\n- NIRAJ RAJNIKANT SHAH: 2 trade(s)\n- GURVINDER SINGH: 2 trade(s)\n- VAGHANI VIRAJ: 1 trade(s)\n- MMD SECURITIES PVT LTD: 1 trade(s)\n- ARCHANA WADHWA: 1 trade(s)\n- ANKITA VISHAL SHAH: 1 trade(s)\n- PREMILA ASHWIN DOSHI: 1 trade(s)\n- JAIN SATISH SAGARMAL: 1 trade(s)\n- MAHENDRAKUMAR S JAIN: 1 trade(s)\n- MAYURI SHRIPAL VORA: 1 trade(s)\n- VANDANA AGARWALA: 1 trade(s)\n- BHASKARA HARINATH PREETHAM: 1 trade(s)\n- FIRST AMONG EQUALS: 1 trade(s)\n- NARESHKUMAR MOHANLAL PATEL: 1 trade(s)\n- ZAINAL KHAN: 1 trade(s)\n- ANKITGERA: 1 trade(s)\n- BRIJESH D PATEL HUF: 1 trade(s)\n- JYOTI K SHAH: 1 trade(s)\n- FAIJAL GAFARBHAI KHATRI: 1 trade(s)\n- SAHAJ TRADING AGENCY: 1 trade(s)\n- NARENDRAKUMAR GANGARAMDAS PATEL: 1 trade(s)\n- DHRUMIL RAKESH PATEL: 1 trade(s)\n- RAKESHBHAI RAMESHBHAI PATEL: 1 trade(s)\n- JUPITER CITY DEVELOPERS (INDIA) LIMITED: 1 trade(s)\n- AKARSHIKA TRADERS LLP: 1 trade(s)\n- TECHNOCRAVES SOLUTIONS PRIVATE LIMITED: 1 trade(s)\n- COMFORT ADVERTISING PVT LTD: 1 trade(s)\n- L7 HITECH PRIVATE LIMITED: 1 trade(s)\n- SHYAMA SHAHI: 1 trade(s)\n- LLOYDS ENTERPRISES LIMITED: 1 trade(s)\n- SEEMA RAGHUNATH AGGARWAL: 1 trade(s)\n- BIPINBHAI DEVABHAI RAVAL: 1 trade(s)\n- SHAH SHARAD KANAYALAL: 1 trade(s)\n- KAMLESHKUMAR JAGDISHBHAI PATEL: 1 trade(s)\n- KISHANKAMLESHKUMARPATEL: 1 trade(s)\n- TULASI MEENA: 1 trade(s)\n- CHIRAG HASMUKH SACHDEV: 1 trade(s)\n- PARMAR CHANDUBHAI: 1 trade(s)\n- DEALMONEY COMMODITIES PRIVATE LIMITED: 1 trade(s)\n- JIGNA MUKESH SHAH: 1 trade(s)\n- JYOTI KHANDELWAL: 1 trade(s)\n- APPLE EQUIFIN PVT LTD: 1 trade(s)\n- NIMIT JAYENDRA SHAH: 1 trade(s)\n- NATVARSINH TAKHUJI CHAVDA: 1 trade(s)\n- DARSHI ATULKUMAR SHAH: 1 trade(s)\n- GIRABEN ATULBHAI SHAH: 1 trade(s)\n- VENKATESH PRASHANTH: 1 trade(s)\n- GOVINDSINGH BHAVNATHSINGH RAJBHAR: 1 trade(s)\n- MANISH RAJPUT: 1 trade(s)\n- MOHAN SINGH SENGAR: 1 trade(s)\n- PARIJATA TRADING PRIVATE LIMITED: 1 trade(s)\n- AVANEESH TRADING PRIVATE LIMITED: 1 trade(s)\n- MONA LAROIA: 1 trade(s)\n- YUGA STOCKS AND COMMODITIES PRIVATE LIMITED: 1 trade(s)\n- RADADIYA CHINTANBHAI ANILBHAI: 1 trade(s)\n- PATEL SHIVLAL KUBERBHAI: 1 trade(s)\n- SAURABH JAIN: 1 trade(s)\n- MANDAKINIBEN PRADYUMANBHAI PATEL: 1 trade(s)\n- UNIQUE SOLUTIONS: 1 trade(s)\n- RAVINDRA HASTIMAL SAKARIYA: 1 trade(s)\n- VEER RAVINDRA SAKARIYA: 1 trade(s)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import requests\nimport pandas as pd\nimport re\nfrom io import StringIO\nfrom datetime import datetime\n\ndef get_bse_bulk_deals():\n    \"\"\"Fetches and parses daily bulk deal data from the BSE website.\"\"\"\n    print(\"Fetching daily bulk deal data...\")\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {'User-Agent': 'Mozilla/5.0'}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        all_tables = pd.read_html(StringIO(response.text))\n        for table in all_tables:\n            if 'Client Name' in table.columns and 'Deal Date' in table.columns:\n                print(\"✅ Bulk deal data fetched.\")\n                table.columns = table.columns.str.strip()\n                # Convert date columns to datetime objects for comparison\n                table['Deal Date'] = pd.to_datetime(table['Deal Date'], format='%d/%m/%Y')\n                return table\n        return pd.DataFrame()\n    except Exception as e:\n        print(f\"❌ Error fetching BSE bulk deal data: {e}\")\n        return pd.DataFrame()\n\ndef parse_chat_for_advisor_mentions(file_path, advisor_name):\n    \"\"\"\n    Parses a chat log to find all stock mentions made by a specific advisor.\n    A stock is assumed to be any word in all caps with 3 or more letters.\n    \"\"\"\n    print(f\"Parsing chat log for messages by '{advisor_name}'...\")\n    stock_mentions = []\n    stock_pattern = re.compile(r'\\b[A-Z]{3,}\\b')\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            # Match lines that contain a date, time, and the specific advisor's name\n            match = re.match(r'(\\d{2}/\\d{2}/\\d{2,4}),\\s\\d{2}:\\d{2}\\s-\\s(.*?):\\s(.*)', line)\n            if match and advisor_name.lower() in match.group(2).lower():\n                date_str, user, message = match.groups()\n                message_date = datetime.strptime(date_str, '%d/%m/%y') # Assuming dd/mm/yy format\n                \n                # Find all potential stock symbols in the message\n                symbols = stock_pattern.findall(message)\n                for symbol in symbols:\n                    stock_mentions.append({\n                        \"date\": message_date,\n                        \"stock_symbol\": symbol,\n                        \"message\": message.strip()\n                    })\n    print(f\"✅ Found {len(stock_mentions)} stock mentions by the advisor.\")\n    return stock_mentions\n\ndef cross_correlation_engine(chat_file, advisor_name):\n    \"\"\"\n    The main engine to correlate chat mentions with bulk trades.\n    \"\"\"\n    # Step 1: Get all recent bulk trades\n    trades_df = get_bse_bulk_deals()\n    if trades_df.empty:\n        return [\"Could not retrieve bulk deal data to perform analysis.\"]\n\n    # Filter for trades made only by our target advisor\n    advisor_trades = trades_df[trades_df['Client Name'].str.contains(advisor_name, case=False, na=False)].copy()\n    if advisor_trades.empty:\n        return [f\"No recent bulk deals found for '{advisor_name}'. No correlation possible.\"]\n    \n    # Step 2: Get all stock mentions by the advisor from the chat log\n    advisor_mentions = parse_chat_for_advisor_mentions(chat_file, advisor_name)\n    if not advisor_mentions:\n        return [f\"No stock mentions found for '{advisor_name}' in the chat log.\"]\n        \n    # Step 3: Correlate mentions and trades to find red flags\n    print(\"\\nCorrelating trades and chat messages...\")\n    red_flags = []\n    \n    for mention in advisor_mentions:\n        for _, trade in advisor_trades.iterrows():\n            # Check if the mentioned stock name is part of the security name in the trade data\n            if mention['stock_symbol'].lower() in trade['Security Name'].lower():\n                time_delta = mention['date'] - trade['Deal Date']\n                \n                # --- Red Flag 1: Front-Running ---\n                # Promotion happens within 7 days AFTER a bulk BUY\n                if trade['Deal Type'].lower() == 'buy' and 0 <= time_delta.days <= 7:\n                    flag = (f\"🚨 POTENTIAL FRONT-RUNNING DETECTED:\\n\"\n                            f\"  -> Advisor promoted '{mention['stock_symbol']}' on {mention['date'].date()}\\n\"\n                            f\"  -> This was just {time_delta.days} day(s) AFTER their bulk BUY of {trade['Quantity']} shares on {trade['Deal Date'].date()}.\\n\")\n                    red_flags.append(flag)\n\n                # --- Red Flag 2: Pump & Dump ---\n                # A bulk SELL happens within 30 days AFTER a promotion\n                if trade['Deal Type'].lower() == 'sell' and 0 <= time_delta.days <= 30:\n                    flag = (f\"🚨 POTENTIAL PUMP & DUMP DETECTED:\\n\"\n                            f\"  -> Advisor made a bulk SELL of {trade['Quantity']} shares of {trade['Security Name']} on {trade['Deal Date'].date()}\\n\"\n                            f\"  -> This was {time_delta.days} day(s) AFTER they promoted '{mention['stock_symbol']}' on {mention['date'].date()}.\\n\")\n                    red_flags.append(flag)\n\n    return red_flags if red_flags else [\"No suspicious correlations found between chat messages and recent bulk deals.\"]\n\nif __name__ == \"__main__\":\n    # --- 🔽 INPUTS FOR THE ANALYSIS 🔽 ---\n    # 1. Path to your chat log file\n    chat_log_file = \"/kaggle/input/whatsap-chats-downloaded/WhatsApp Chat with TriDevs.txt\" #<-- CHANGE THIS\n    \n    # 2. The exact name of the advisor as it appears in the chat and bulk deal data\n    advisor_name_to_investigate = \"NEO APEX VENTURE LLP\" #<-- CHANGE THIS\n    \n    # --- Run the engine ---\n    if not os.path.exists(chat_log_file):\n        print(f\"❌ ERROR: Chat file not found at '{chat_log_file}'\")\n    else:\n        suspicious_activities = cross_correlation_engine(chat_log_file, advisor_name_to_investigate)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"          Cross-Correlation Analysis Report\")\n        print(\"=\"*80)\n        for activity in suspicious_activities:\n            print(activity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:28.353543Z","iopub.execute_input":"2025-09-10T04:31:28.353981Z","iopub.status.idle":"2025-09-10T04:31:28.501907Z","shell.execute_reply.started":"2025-09-10T04:31:28.353953Z","shell.execute_reply":"2025-09-10T04:31:28.501315Z"}},"outputs":[{"name":"stdout","text":"Fetching daily bulk deal data...\n✅ Bulk deal data fetched.\nParsing chat log for messages by 'NEO APEX VENTURE LLP'...\n✅ Found 0 stock mentions by the advisor.\n\n================================================================================\n          Cross-Correlation Analysis Report\n================================================================================\nNo stock mentions found for 'NEO APEX VENTURE LLP' in the chat log.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n!pip install pandas scikit-learn joblib opencv-python easyocr python-whois requests beautifulsoup4 html5lib yfinance -q\n\n\nimport os\nimport re\nimport glob\nimport json\nimport datetime\nfrom collections import Counter, defaultdict\nfrom urllib.parse import urlparse\nfrom io import StringIO\n\n# --- Suppress Warnings ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Core Data Science & ML Imports ---\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# --- Image & Web Scraping Imports ---\nimport cv2\nimport easyocr\nimport whois\nimport requests\nimport yfinance as yf\n\nprint(\"✅ All libraries loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:58:50.358251Z","iopub.execute_input":"2025-09-20T17:58:50.358662Z","iopub.status.idle":"2025-09-20T18:00:22.106086Z","shell.execute_reply.started":"2025-09-20T17:58:50.358626Z","shell.execute_reply":"2025-09-20T18:00:22.104861Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h✅ All libraries loaded successfully.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\ndef train_scam_classifier():\n    \"\"\"\n    Loads the dataset, trains a text classifier, and saves the model\n    and vectorizer to the /kaggle/working/ directory.\n    \"\"\"\n    print(\"🚀 Starting model training process...\")\n    try:\n        # Define paths for the writable Kaggle directory\n        WORKING_DIR = \"/kaggle/working/\"\n        VECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\n        MODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n\n        # Load dataset\n        df = pd.read_csv(\"/kaggle/input/whatsapp-scam/whatsapp_scam_dataset.csv\")\n\n        # Define features & labels\n        X = df[\"message\"]\n        y = df[\"scam_type\"]\n\n        # Train-test split\n        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n        # Text vectorization\n        vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000, ngram_range=(1,2))\n        X_train_vec = vectorizer.fit_transform(X_train)\n\n        # Train model\n        model = LogisticRegression(max_iter=200, class_weight=\"balanced\")\n        model.fit(X_train_vec, y_train)\n\n        # Save the model and vectorizer\n        joblib.dump(vectorizer, VECTORIZER_PATH)\n        joblib.dump(model, MODEL_PATH)\n        \n        print(f\"✅ Model and vectorizer saved successfully to {WORKING_DIR}\")\n        return True\n\n    except Exception as e:\n        print(f\"❌ An error occurred during model training: {e}\")\n        print(\"    Ensure '/kaggle/input/whatsapp-scam/whatsapp_scam_dataset.csv' is added to your notebook.\")\n        return False\n\n# Run the training\ntrain_scam_classifier()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T18:00:22.107772Z","iopub.execute_input":"2025-09-20T18:00:22.108293Z","iopub.status.idle":"2025-09-20T18:00:23.851608Z","shell.execute_reply.started":"2025-09-20T18:00:22.108260Z","shell.execute_reply":"2025-09-20T18:00:23.850765Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting model training process...\n✅ Model and vectorizer saved successfully to /kaggle/working/\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 3: COMPILED ANALYSIS PIPELINE\n# ==============================================================================\n\n# --- Helper Function: URL Intelligence ---\ndef check_url_risk(url):\n    score, reasons = 0, []\n    try:\n        domain = urlparse(url).netloc\n        if not url.startswith(\"https\"): score += 30; reasons.append(\"URL is not secure (HTTP)\")\n        if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain): score += 40; reasons.append(\"URL uses an IP address\")\n        if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]): score += 25; reasons.append(\"URL uses a suspicious TLD\")\n        try:\n            domain_info = whois.whois(domain)\n            creation_date = domain_info.creation_date\n            if creation_date:\n                creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                age_days = (datetime.datetime.now() - creation_date).days\n                if age_days < 180: score += 30; reasons.append(f\"Domain is very new ({age_days} days old)\")\n        except Exception: score += 10; reasons.append(\"WHOIS lookup failed\")\n    except Exception: return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL is malformed\"]}\n    risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n    return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n# --- Helper Function: Image & QR Intelligence ---\ndef check_qr_code(image_path):\n    try:\n        img = cv2.imread(image_path)\n        if img is None: return None\n        qr_detector = cv2.QRCodeDetector()\n        data, _, _ = qr_detector.detectAndDecode(img)\n        if data: return {\"qr_data\": data, \"analysis\": check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]}}\n    except Exception: return None\n    return None\n\ndef analyze_image_content(image_path, reader, text_model, text_vectorizer):\n    results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": check_qr_code(image_path)}\n    try:\n        extracted_text = \" \".join(reader.readtext(image_path, detail=0, paragraph=True))\n        if extracted_text.strip():\n            results[\"ocr_text\"] = extracted_text\n            vec = text_vectorizer.transform([extracted_text])\n            results[\"text_scam_prediction\"] = text_model.predict(vec)[0]\n    except Exception: pass\n    return results\n\n# --- Helper Function: SEBI & Stock Mention (Placeholders) ---\ndef verify_sebi_id(sebi_id):\n    if not sebi_id or not sebi_id.strip(): return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n    print(f\"⚠️  Running placeholder for SEBI ID: {sebi_id}\")\n    return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\ndef analyze_stock_mentions(text):\n    stock_symbols = re.findall(r'\\b[A-Z]{3,}\\b', text)\n    if not stock_symbols: return None\n    suspicious_patterns = [\"Message contains pump-and-dump keywords.\"] if any(kw in text.lower() for kw in [\"guaranteed\", \"10x\", \"insider\"]) else []\n    return {\"mentioned_stocks\": list(set(stock_symbols)), \"suspicious_patterns\": suspicious_patterns}\n\n# --- Helper Function: Chat Analysis ---\ndef parse_chat_for_advisor_mentions(file_path, advisor_name):\n    stock_mentions, stock_pattern = [], re.compile(r'\\b[A-Z]{3,}\\b')\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            match = re.match(r'(\\d{2}/\\d{2}/\\d{2,4}),\\s\\d{2}:\\d{2}\\s-\\s(.*?):\\s(.*)', line)\n            if match and advisor_name.lower() in match.group(2).lower():\n                date_str, _, message = match.groups()\n                try: message_date = datetime.datetime.strptime(date_str, '%d/%m/%y')\n                except ValueError: message_date = datetime.datetime.strptime(date_str, '%d/%m/%Y')\n                for symbol in stock_pattern.findall(message):\n                    stock_mentions.append({\"date\": message_date, \"stock_symbol\": symbol, \"message\": message.strip()})\n    return stock_mentions\n\ndef detect_bot_behavior(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f: lines = f.readlines()\n    users = [re.match(r'.*?-\\s(.*?):', line).group(1).strip() for line in lines if re.match(r'.*?-\\s(.*?):', line)]\n    if not users: return {\"error\": \"No users found.\"}\n    user_counts = Counter(users)\n    admin_name = user_counts.most_common(1)[0][0]\n    return {\"detected_admin\": admin_name, \"message_counts\": dict(user_counts)}\n\n# --- Helper Function: Bulk Deal & Cross-Correlation ---\ndef get_bse_bulk_deals():\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {'User-Agent': 'Mozilla/5.0'}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        for table in pd.read_html(StringIO(response.text)):\n            if 'Client Name' in table.columns:\n                table.columns = table.columns.str.strip()\n                table['Deal Date'] = pd.to_datetime(table['Deal Date'], format='%d/%m/%Y')\n                return table\n    except Exception: return pd.DataFrame()\n    return pd.DataFrame()\n\ndef cross_correlation_engine(chat_file, advisor_name, trades_df):\n    if trades_df.empty or not advisor_name: return [\"Bulk deal data not available or no advisor name provided.\"]\n    advisor_trades = trades_df[trades_df['Client Name'].str.contains(advisor_name, case=False, na=False)]\n    if advisor_trades.empty: return [f\"No bulk deals found for '{advisor_name}'.\"]\n    advisor_mentions = parse_chat_for_advisor_mentions(chat_file, advisor_name)\n    if not advisor_mentions: return [f\"No stock mentions by '{advisor_name}' found in chat.\"]\n    red_flags = []\n    for mention in advisor_mentions:\n        for _, trade in advisor_trades.iterrows():\n            if mention['stock_symbol'].lower() in trade['Security Name'].lower():\n                time_delta = mention['date'] - trade['Deal Date']\n                if trade['Deal Type'].lower() == 'buy' and 0 <= time_delta.days <= 7:\n                    red_flags.append(f\"🚨 POTENTIAL FRONT-RUNNING: Advisor promoted '{mention['stock_symbol']}' on {mention['date'].date()} just {time_delta.days} day(s) AFTER a bulk BUY.\")\n                if trade['Deal Type'].lower() == 'sell' and 0 <= time_delta.days <= 30:\n                    red_flags.append(f\"🚨 POTENTIAL PUMP & DUMP: Advisor made a bulk SELL of {trade['Security Name']} {time_delta.days} day(s) AFTER promoting '{mention['stock_symbol']}'.\")\n    return red_flags if red_flags else [\"No suspicious correlations found.\"]\n\n# --- THE MAIN PIPELINE FUNCTION ---\ndef run_analysis_pipeline(chat_file_path, image_folder_path, sebi_id_to_check, entity_name_to_track):\n    \"\"\"Orchestrates the entire fraud detection and analysis process.\"\"\"\n    print(\"🚀 Starting Full Analysis Pipeline...\")\n    WORKING_DIR = \"/kaggle/working/\"\n    VECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\n    MODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n    try:\n        vectorizer = joblib.load(VECTORIZER_PATH)\n        model = joblib.load(MODEL_PATH)\n        reader = easyocr.Reader(['en'], gpu=False) # Set gpu=False for Kaggle CPU environment\n    except Exception as e: return {\"error\": f\"Failed to load models or OCR: {e}\"}\n\n    final_report = defaultdict(dict)\n    \n    # 1. Perform one-time analyses\n    print(\"📈 Analyzing market and chat-level data...\")\n    final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = verify_sebi_id(sebi_id_to_check)\n    bulk_deals_df = get_bse_bulk_deals()\n    final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n    final_report[\"cross_correlation_analysis\"] = cross_correlation_engine(chat_file_path, entity_name_to_track, bulk_deals_df)\n    final_report[\"chat_analysis\"] = detect_bot_behavior(chat_file_path)\n\n    # 2. Perform message-by-message and image analysis\n    print(\"🔬 Analyzing individual messages and images...\")\n    messages = parse_chat_for_advisor_mentions(chat_file_path, \"\") # Get all messages\n    scam_predictions = []\n    final_report[\"message_by_message_analysis\"] = []\n    for msg_data in messages:\n        text = msg_data[\"message\"]\n        analysis = {\"text_scam_prediction\": model.predict(vectorizer.transform([text]))[0]}\n        scam_predictions.append(analysis[\"text_scam_prediction\"])\n        if re.search(r'(https?://\\S+)', text):\n            analysis[\"url_analysis\"] = [check_url_risk(url) for url in re.findall(r'(https?://\\S+)', text)]\n        if analyze_stock_mentions(text):\n            analysis[\"stock_mention_analysis\"] = analyze_stock_mentions(text)\n        if len(analysis) > 1:\n            final_report[\"message_by_message_analysis\"].append({\"message\": text, \"analysis\": analysis})\n    \n    image_files = glob.glob(os.path.join(image_folder_path, \"*[.png|.jpg|.jpeg]\"))\n    final_report[\"image_analysis\"] = []\n    for img_path in image_files:\n        final_report[\"image_analysis\"].append({\"file_name\": os.path.basename(img_path), \"analysis\": analyze_image_content(img_path, reader, model, vectorizer)})\n\n    # 3. Compile summary\n    final_report[\"summary\"] = {\n        \"total_messages_analyzed\": len(messages),\n        \"total_images_analyzed\": len(image_files),\n        \"scam_types_detected\": dict(Counter(p for p in scam_predictions if p != \"not_scam\"))\n    }\n    print(\"\\n✅ Pipeline finished successfully!\")\n    return final_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:16:02.130463Z","iopub.execute_input":"2025-09-15T04:16:02.130697Z","iopub.status.idle":"2025-09-15T04:16:02.153686Z","shell.execute_reply.started":"2025-09-15T04:16:02.130680Z","shell.execute_reply":"2025-09-15T04:16:02.153023Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os\nchat_file = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/chat/Extracted messages....txt\"\n\nimage_folder = \"/kaggle/input/scam-qr-png/\"\n\nsebi_id_input = \"INZ000048660\"\n\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\nif os.path.exists(chat_file):\n    results = run_analysis_pipeline(\n        chat_file_path=chat_file,\n        image_folder_path=image_folder,\n        sebi_id_to_check=sebi_id_input,\n        entity_name_to_track=entity_name_input\n    )\n    \n    # Print the final report as a clean JSON\n    print(\"\\n\" + \"=\"*50)\n    print(\"          FINAL FRAUD DETECTION REPORT\")\n    print(\"=\"*50 + \"\\n\")\n    print(json.dumps(results, indent=4))\nelse:\n    print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:16:02.154443Z","iopub.execute_input":"2025-09-15T04:16:02.154720Z","iopub.status.idle":"2025-09-15T04:16:14.054416Z","shell.execute_reply.started":"2025-09-15T04:16:02.154703Z","shell.execute_reply":"2025-09-15T04:16:14.053654Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Full Analysis Pipeline...\n📈 Analyzing market and chat-level data...\n⚠️  Running placeholder for SEBI ID: INZ000048660\n🔬 Analyzing individual messages and images...\n\n✅ Pipeline finished successfully!\n\n==================================================\n          FINAL FRAUD DETECTION REPORT\n==================================================\n\n{\n    \"manual_verifications\": {\n        \"sebi_id_analysis\": {\n            \"sebi_id\": \"INZ000048660\",\n            \"status\": \"Not Found (Dummy)\"\n        }\n    },\n    \"bulk_deal_analysis\": {\n        \"trades_found_today\": 123\n    },\n    \"cross_correlation_analysis\": [\n        \"No stock mentions by 'NEO APEX VENTURE LLP' found in chat.\"\n    ],\n    \"chat_analysis\": {\n        \"detected_admin\": \"vibhugupta555\",\n        \"message_counts\": {\n            \"Axin852000\": 6,\n            \"Cryptohackfi\": 28,\n            \"kalamisg010\": 8,\n            \"Dabba_cliets_data_provider\": 11,\n            \"Vikas_9010\": 14,\n            \"None\": 115,\n            \"MEPAYhao\": 6,\n            \"cryptus455\": 25,\n            \"vibhugupta555\": 201,\n            \"Mcxgoldliveresearchrj\": 34,\n            \"Code4Market\": 11,\n            \"Vikas_9012\": 23,\n            \"futurerobo2022\": 4,\n            \"lxd6695\": 1,\n            \"parinit\": 12,\n            \"LiladharGaur\": 1,\n            \"yd888818\": 36,\n            \"Flash_D_Best\": 21,\n            \"sihag6\": 8,\n            \"wealth_creator_admin\": 29,\n            \"Ungraduate_traders\": 26,\n            \"pranavsoni22\": 14,\n            \"AP_Upstox\": 11,\n            \"Contact2hunter\": 21,\n            \"Ankurlearner\": 3,\n            \"Treasurechart\": 2,\n            \"Gauri_itc\": 9,\n            \"AVl_TRADER\": 6,\n            \"ACEPAYJG\": 1,\n            \"MR_SAlNT\": 5,\n            \"AlgoDevStudio\": 7,\n            \"Rishabh889\": 2,\n            \"RAGHUVANSHI5\": 17,\n            \"Trader_for_you79\": 12,\n            \"StoxTAdmin\": 4,\n            \"Dhamojadugar\": 4,\n            \"jayrajahir11\": 1,\n            \"Freeearninggame2\": 4,\n            \"VikasbankiftyNG\": 4,\n            \"Ineedwork5464\": 1,\n            \"HJ_Talks\": 2,\n            \"Yasmin9155\": 3,\n            \"itallio\": 1\n        }\n    },\n    \"message_by_message_analysis\": [],\n    \"image_analysis\": [\n        {\n            \"file_name\": \"WhatsApp Image 2025-09-09 at 13.07.59_522543d6.jpg\",\n            \"analysis\": {\n                \"ocr_text\": \"PhonePe ACCEPTED HERE Scan & Pay Using PhonePe App 4 Rajarshi Somvanshi 2025, All rights reserved, PhonePe Ltd (Formerly known as 'PhonePe Private Ltd')\",\n                \"text_scam_prediction\": \"Fake Job Offer Scam\",\n                \"qr_analysis\": null\n            }\n        }\n    ],\n    \"summary\": {\n        \"total_messages_analyzed\": 0,\n        \"total_images_analyzed\": 1,\n        \"scam_types_detected\": {}\n    }\n}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport joblib\nimport whois\nimport cv2\nimport easyocr\nimport requests\nimport datetime\nimport pandas as pd\nfrom io import StringIO\nfrom urllib.parse import urlparse\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport glob\n\n\nclass FraudDetectionPipeline:\n    \"\"\"\n    A self-contained class to run the multi-modal scam detection pipeline.\n    \"\"\"\n\n    def __init__(self, model_path, vectorizer_path):\n        \"\"\"Load ML model, vectorizer, and OCR reader.\"\"\"\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False)  # always CPU in Kaggle\n            print(\"✅ Pipeline initialized successfully.\")\n        except Exception as e:\n            raise IOError(\n                f\"❌ Failed to load models. Ensure files exist at the specified paths. Error: {e}\"\n            )\n\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"):\n                score += 30\n                reasons.append(\"URL is not secure (HTTP)\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain):\n                score += 40\n                reasons.append(\"URL uses an IP address\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]):\n                score += 25\n                reasons.append(\"URL uses a suspicious TLD\")\n\n            try:\n                domain_info = whois.whois(domain)\n                creation_date = domain_info.creation_date\n                if creation_date:\n                    creation_date = (\n                        creation_date[0]\n                        if isinstance(creation_date, list)\n                        else creation_date\n                    )\n                    age_days = (datetime.datetime.now() - creation_date).days\n                    if age_days < 180:\n                        score += 30\n                        reasons.append(f\"Domain is very new ({age_days} days old)\")\n            except Exception:\n                score += 10\n                reasons.append(\"WHOIS lookup failed\")\n\n        except Exception:\n            return {\n                \"url\": url,\n                \"risk_level\": \"High\",\n                \"risk_score\": 100,\n                \"reasons\": [\"URL is malformed\"],\n            }\n\n        risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\n            \"url\": url,\n            \"risk_level\": risk_level,\n            \"risk_score\": min(score, 100),\n            \"reasons\": reasons,\n        }\n\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None:\n                return None\n            qr_detector = cv2.QRCodeDetector()\n            data, _, _ = qr_detector.detectAndDecode(img)\n            if data:\n                return {\n                    \"qr_data\": data,\n                    \"analysis\": self._check_url_risk(data)\n                    if data.startswith(\"http\")\n                    else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]},\n                }\n        except Exception:\n            return None\n        return None\n\n    def _analyze_image_content(self, image_path):\n        results = {\n            \"ocr_text\": \"\",\n            \"text_scam_prediction\": \"N/A\",\n            \"qr_analysis\": self._check_qr_code(image_path),\n        }\n        try:\n            extracted_text = \" \".join(\n                self.ocr_reader.readtext(image_path, detail=0, paragraph=True)\n            )\n            if extracted_text.strip():\n                results[\"ocr_text\"] = extracted_text\n                vec = self.vectorizer.transform([extracted_text])\n                results[\"text_scam_prediction\"] = self.model.predict(vec)[0]\n        except Exception:\n            pass\n        return results\n\n\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id or not sebi_id.strip():\n            return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n        print(f\"⚠️  Running placeholder for SEBI ID: {sebi_id}\")\n        return {\n            \"sebi_id\": sebi_id,\n            \"status\": \"Verified (Dummy)\"\n            if len(sebi_id) > 10 and \"INA\" in sebi_id\n            else \"Not Found (Dummy)\",\n        }\n\n    def _analyze_stock_mentions(self, text):\n        stock_symbols = re.findall(r\"\\b[A-Z]{3,}\\b\", text)\n        if not stock_symbols:\n            return None\n        suspicious_patterns = (\n            [\"Message contains pump-and-dump keywords.\"]\n            if any(kw in text.lower() for kw in [\"guaranteed\", \"10x\", \"insider\"])\n            else []\n        )\n        return {\n            \"mentioned_stocks\": list(set(stock_symbols)),\n            \"suspicious_patterns\": suspicious_patterns,\n        }\n\n    def _parse_chat_for_messages(self, chat_file_path, user_filter=\"\"):\n        messages = []\n        # Updated regex for your dataset format\n        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}[+]\\d{2}:\\d{2}) - (.*?): (.*)')\n    \n        with open(chat_file_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                match = pattern.match(line.strip())\n                if match:\n                    timestamp, user, message = match.groups()\n                    if user_filter == \"\" or user_filter.lower() in user.lower():\n                        messages.append({\n                            \"date\": timestamp,\n                            \"user\": user,\n                            \"message\": message\n                        })\n        return messages\n\n    def _detect_bot_behavior(self, file_path):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        users = [\n            re.match(r\".*?-\\s(.*?):\", line).group(1).strip()\n            for line in lines\n            if re.match(r\".*?-\\s(.*?):\", line)\n        ]\n        if not users:\n            return {\"error\": \"No users found.\"}\n        user_counts = Counter(users)\n        admin_name = user_counts.most_common(1)[0][0]\n        return {\"detected_admin\": admin_name, \"message_counts\": dict(user_counts)}\n\n    def _get_bse_bulk_deals(self):\n        try:\n            url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            for table in pd.read_html(StringIO(response.text)):\n                if \"Client Name\" in table.columns:\n                    table.columns = table.columns.str.strip()\n                    table[\"Deal Date\"] = pd.to_datetime(\n                        table[\"Deal Date\"], format=\"%d/%m/%Y\"\n                    )\n                    return table\n        except Exception:\n            return pd.DataFrame()\n        return pd.DataFrame()\n\n    def _cross_correlation_engine(self, chat_file, advisor_name, trades_df):\n        if trades_df.empty or not advisor_name:\n            return [\"Bulk deal data not available or no advisor name provided.\"]\n\n        advisor_trades = trades_df[\n            trades_df[\"Client Name\"].str.contains(advisor_name, case=False, na=False)\n        ]\n        if advisor_trades.empty:\n            return [f\"No bulk deals found for '{advisor_name}'.\"]\n\n        advisor_mentions = self._parse_chat_for_messages(chat_file, user_filter=advisor_name)\n        if not advisor_mentions:\n            return [f\"No stock mentions by '{advisor_name}' found in chat.\"]\n\n        red_flags = []\n        for mention in advisor_mentions:\n            for symbol in mention[\"stock_symbols\"]:\n                for _, trade in advisor_trades.iterrows():\n                    if symbol.lower() in trade[\"Security Name\"].lower():\n                        time_delta = mention[\"date\"] - trade[\"Deal Date\"]\n                        if trade[\"Deal Type\"].lower() == \"buy\" and 0 <= time_delta.days <= 7:\n                            red_flags.append(\n                                f\"🚨 POTENTIAL FRONT-RUNNING: Advisor promoted '{symbol}' on {mention['date'].date()} just {time_delta.days} day(s) AFTER a bulk BUY.\"\n                            )\n                        if trade[\"Deal Type\"].lower() == \"sell\" and 0 <= time_delta.days <= 30:\n                            red_flags.append(\n                                f\"🚨 POTENTIAL PUMP & DUMP: Advisor made a bulk SELL of {trade['Security Name']} {time_delta.days} day(s) AFTER promoting '{symbol}'.\"\n                            )\n        return red_flags if red_flags else [\"No suspicious correlations found.\"]\n\n    def _check_insider_trading(self, trades_df, insider_list, scam_messages):\n        if trades_df.empty:\n            return [\"No bulk deal data available.\"]\n        \n        red_flags = []\n        for _, trade in trades_df.iterrows():\n            client = trade[\"Client Name\"].lower()\n            for insider in insider_list:\n                if insider.lower() in client:\n                    # Check overlap with scam messages\n                    for msg in scam_messages:\n                        for stock in msg.get(\"analysis\", {}).get(\"stock_mention_analysis\", {}).get(\"mentioned_stocks\", []):\n                            if stock.lower() in trade[\"Security Name\"].lower():\n                                red_flags.append(\n                                    f\"🚨 Insider {trade['Client Name']} traded {trade['Security Name']} \"\n                                    f\"({trade['Deal Type']}) while scam messages circulated.\"\n                                )\n        return red_flags if red_flags else [\"No insider red flags detected.\"]\n\n        from transformers import pipeline\n    import yfinance as yf\n\n    def _sentiment_price_check(self, scam_messages):\n        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n        alerts = []\n        for msg in scam_messages:\n            text = msg[\"message\"]\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not stock_info:\n                continue\n            \n            sentiment = sentiment_analyzer(text[:250])[0]  # truncate long texts\n            label = sentiment[\"label\"].lower()\n            for stock in stock_info[\"mentioned_stocks\"]:\n                try:\n                    data = yf.download(stock + \".NS\", period=\"5d\", interval=\"1d\")\n                    if not data.empty:\n                        change = data[\"Close\"].iloc[-1] - data[\"Close\"].iloc[-2]\n                        if \"positive\" in label and change < 0:\n                            alerts.append(f\"🚨 Positive sentiment for {stock} but price fell → suspicious hype.\")\n                        if \"negative\" in label and change > 0:\n                            alerts.append(f\"🚨 Negative sentiment for {stock} but price rose → suspicious suppression.\")\n                except Exception:\n                    continue\n        return alerts if alerts else [\"No suspicious sentiment/price mismatches.\"]\n\n\n    def _social_trading_correlation(self, stock_symbols):\n        # 🔎 Mock implementation — replace with Twitter/Telegram APIs\n        trending_stocks = {\"XYZ\": [\"#XYZstock trending on Twitter\"], \"ABC\": [\"Telegram hype detected\"]}\n        \n        alerts = []\n        for stock in stock_symbols:\n            if stock in trending_stocks:\n                alerts.append(\n                    f\"🚨 Coordinated manipulation risk: {stock} is trending online AND in bulk deals.\"\n                )\n        return alerts if alerts else [\"No social + trading manipulation risk found.\"]\n        \n\n    def _portfolio_risk_alerts(self, holdings, scam_messages, trades_df):\n        alerts = []\n        for stock in holdings:\n            for msg in scam_messages:\n                stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n                if stock_info and stock in stock_info[\"mentioned_stocks\"]:\n                    alerts.append(f\"🚨 Scam messages detected for your holding: {stock}.\")\n            if not trades_df.empty and trades_df[\"Security Name\"].str.contains(stock, case=False).any():\n                alerts.append(f\"🚨 Bulk deals detected for your holding: {stock}.\")\n        return alerts if alerts else [\"No risks detected for portfolio.\"]\n\n    \n    \n    def run(self, chat_file_path, image_folder_path, sebi_id_to_check, entity_name_to_track):\n        \"\"\"Run the full fraud detection pipeline.\"\"\"\n        print(\"🚀 Starting Full Analysis Pipeline...\")\n        final_report = defaultdict(dict)\n\n        # 1. Chat-level + market analysis\n        print(\"📈 Analyzing market and chat-level data...\")\n        final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = self._verify_sebi_id(sebi_id_to_check)\n        bulk_deals_df = self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n        final_report[\"cross_correlation_analysis\"] = self._cross_correlation_engine(\n            chat_file_path, entity_name_to_track, bulk_deals_df\n        )\n        final_report[\"chat_analysis\"] = self._detect_bot_behavior(chat_file_path)\n\n        # 2. Message-by-message + image analysis\n        print(\"🔬 Analyzing individual messages and images...\")\n        all_messages = self._parse_chat_for_messages(chat_file_path, user_filter=\"\")  # all users\n        scam_predictions = []\n        final_report[\"message_by_message_analysis\"] = []\n\n        for msg_data in all_messages:\n            text = msg_data[\"message\"]\n            analysis = {\n                \"text_scam_prediction\": self.model.predict(self.vectorizer.transform([text]))[0]\n            }\n            scam_predictions.append(analysis[\"text_scam_prediction\"])\n\n            if re.search(r\"(https?://\\S+)\", text):\n                analysis[\"url_analysis\"] = [\n                    self._check_url_risk(url) for url in re.findall(r\"(https?://\\S+)\", text)\n                ]\n\n            stock_analysis = self._analyze_stock_mentions(text)\n            if stock_analysis:\n                analysis[\"stock_mention_analysis\"] = stock_analysis\n\n            final_report[\"message_by_message_analysis\"].append(\n                {\n                    \"date\": msg_data[\"date\"],\n                    \"user\": msg_data[\"user\"],\n                    \"message\": text,\n                    \"analysis\": analysis,\n                }\n            )\n\n        # 3. Image analysis\n        image_files = (\n            glob.glob(os.path.join(image_folder_path, \"*.png\"))\n            + glob.glob(os.path.join(image_folder_path, \"*.jpg\"))\n            + glob.glob(os.path.join(image_folder_path, \"*.jpeg\"))\n        )\n        final_report[\"insider_trading_flags\"] = self._check_insider_trading(\n            bulk_deals_df, insider_list=[\"Promoter XYZ\", \"Director ABC\"], \n            scam_messages=final_report[\"message_by_message_analysis\"]\n        )\n\n        final_report[\"sentiment_price_flags\"] = self._sentiment_price_check(\n            final_report[\"message_by_message_analysis\"]\n        )\n\n\n        mentioned_stocks = set()\n        for msg in final_report[\"message_by_message_analysis\"]:\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if stock_info:\n                mentioned_stocks.update(stock_info[\"mentioned_stocks\"])\n        \n        final_report[\"social_trading_flags\"] = self._social_trading_correlation(list(mentioned_stocks))\n\n        user_holdings = [\"XYZ\", \"ABC\"]  # Example\n        final_report[\"portfolio_alerts\"] = self._portfolio_risk_alerts(\n            user_holdings,\n            final_report[\"message_by_message_analysis\"],\n            bulk_deals_df\n        )\n\n        final_report[\"image_analysis\"] = []\n        for img_path in image_files:\n            final_report[\"image_analysis\"].append(\n                {\n                    \"file_name\": os.path.basename(img_path),\n                    \"analysis\": self._analyze_image_content(img_path),\n                }\n            )\n\n        # 4. Summary\n        final_report[\"summary\"] = {\n            \"total_messages_analyzed\": len(all_messages),\n            \"total_images_analyzed\": len(image_files),\n            \"scam_types_detected\": dict(\n                Counter(p for p in scam_predictions if p != \"not_scam\")\n            ),\n        }\n\n        print(\"\\n✅ Pipeline finished successfully!\")\n        return final_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:49:14.424526Z","iopub.execute_input":"2025-09-15T07:49:14.424800Z","iopub.status.idle":"2025-09-15T07:49:14.450397Z","shell.execute_reply.started":"2025-09-15T07:49:14.424781Z","shell.execute_reply":"2025-09-15T07:49:14.449570Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"WORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n\n# Input data paths\nchat_file = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/chat/Extracted messages....txt\"\nimage_folder = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/images\"\n\n# Entity information\nsebi_id_input = \"INZ000048660\"\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\n\n\ntry:\n  \n    fraud_detector = FraudDetectionPipeline(model_path=MODEL_PATH, vectorizer_path=VECTORIZER_PATH)\n    \n   \n    if os.path.exists(chat_file):\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=sebi_id_input,\n            entity_name_to_track=entity_name_input\n        )\n        \n        # 3. Print the final report.\n        print(\"\\n\" + \"=\"*50)\n        print(\"                 FINAL FRAUD DETECTION REPORT\")\n        print(\"=\"*50 + \"\\n\")\n        print(json.dumps(results, indent=4, default=str))\n    else:\n        print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")\n\nexcept IOError as e:\n    print(e)\n    print(\"Please run the 'train_scam_classifier()' function first to generate model files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nstock_df = pd.read_csv(\"/kaggle/input/nse-stocks/EQUITY_L.csv\")  # Replace with your CSV path\n\n# Standardize columns\nstock_df.columns = [col.strip() for col in stock_df.columns]\n\n# Create sets for fast lookup\nticker_set = set(stock_df['SYMBOL'].str.upper())\ncompany_name_map = dict(zip(stock_df['NAME OF COMPANY'].str.lower(), stock_df['SYMBOL'].str.upper()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:48:32.797045Z","iopub.execute_input":"2025-09-20T12:48:32.797768Z","iopub.status.idle":"2025-09-20T12:48:32.819139Z","shell.execute_reply.started":"2025-09-20T12:48:32.797731Z","shell.execute_reply":"2025-09-20T12:48:32.818594Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\ninsider_df = pd.read_csv(\"/kaggle/input/insider-trading/CF-Insider-Trading-equities-20-Sep-2025.csv\")  # your downloaded file\n\n# Standardize column names\ninsider_df.columns = [col.strip() for col in insider_df.columns]\n\n# Convert date columns to datetime\ninsider_df['DATE OF ALLOTMENT/ACQUISITION FROM'] = pd.to_datetime(insider_df['DATE OF ALLOTMENT/ACQUISITION FROM'], errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T12:53:04.778887Z","iopub.execute_input":"2025-09-20T12:53:04.779177Z","iopub.status.idle":"2025-09-20T12:53:04.802587Z","shell.execute_reply.started":"2025-09-20T12:53:04.779155Z","shell.execute_reply":"2025-09-20T12:53:04.801739Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport joblib\nimport whois\nimport cv2\nimport easyocr\nimport requests\nimport datetime\nimport pandas as pd\nfrom io import StringIO\nfrom urllib.parse import urlparse\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport glob\nfrom transformers import pipeline\nimport yfinance as yf\n\nclass FraudDetectionPipeline:\n    \"\"\"\n    Multi-modal scam detection pipeline.\n    \"\"\"\n\n    def __init__(self, model_path, vectorizer_path, stock_csv_path):\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False)\n            print(\"✅ Pipeline initialized successfully.\")\n        except Exception as e:\n            raise IOError(f\"❌ Failed to load models. Error: {e}\")\n\n        # Load stock list\n        stock_df = pd.read_csv(stock_csv_path)\n        self.ticker_set = set(stock_df[\"SYMBOL\"].str.upper().tolist())\n        self.company_name_map = dict(zip(\n            stock_df[\"NAME OF COMPANY\"].str.lower(),\n            stock_df[\"SYMBOL\"].str.upper()\n        ))\n\n    # --- URL Risk ---\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"): score += 30; reasons.append(\"URL is not secure (HTTP)\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain): score += 40; reasons.append(\"URL uses an IP address\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]): score += 25; reasons.append(\"Suspicious TLD\")\n            try:\n                info = whois.whois(domain)\n                cd = info.creation_date\n                if cd:\n                    cd = cd[0] if isinstance(cd, list) else cd\n                    if (datetime.datetime.now() - cd).days < 180:\n                        score += 30\n                        reasons.append(f\"Domain is very new ({(datetime.datetime.now() - cd).days} days)\")\n            except: score += 10; reasons.append(\"WHOIS lookup failed\")\n        except: return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL malformed\"]}\n        level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\"url\": url, \"risk_level\": level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n    # --- QR Code ---\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None: return None\n            qr = cv2.QRCodeDetector()\n            data, _, _ = qr.detectAndDecode(img)\n            if data:\n                return {\n                    \"qr_data\": data,\n                    \"analysis\": self._check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\":\"Low\",\"reasons\":[\"Non-URL text\"]}\n                }\n        except: return None\n        return None\n\n    # --- Image Content Analysis ---\n    def _analyze_image_content(self, image_path):\n        results = {\"ocr_text\": \"\", \"text_scam_prediction\":\"N/A\", \"qr_analysis\": self._check_qr_code(image_path)}\n        try:\n            text = \" \".join(self.ocr_reader.readtext(image_path, detail=0, paragraph=True))\n            if text.strip():\n                results[\"ocr_text\"] = text\n                results[\"text_scam_prediction\"] = self.model.predict(self.vectorizer.transform([text]))[0]\n        except: pass\n        return results\n\n    # --- SEBI ID ---\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id or not sebi_id.strip(): return {\"sebi_id\":\"Not Provided\",\"status\":\"N/A\"}\n        return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id)>10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\n    # --- Stock Detection ---\n    def detect_stocks_in_text(self, text):\n        found, text_lower = set(), text.lower()\n        for name, symbol in self.company_name_map.items():\n            if name in text_lower: found.add(symbol)\n        for w in re.findall(r'\\b\\w+\\b', text):\n            if w.upper() in self.ticker_set: found.add(w.upper())\n        return list(found)\n\n    def _analyze_stock_mentions(self, text):\n        stocks = self.detect_stocks_in_text(text)\n        if not stocks: return None\n        suspicious = [\"Message contains pump-and-dump keywords.\"] if any(k in text.lower() for k in [\"guaranteed\",\"10x\",\"insider\"]) else []\n        return {\"mentioned_stocks\": stocks, \"suspicious_patterns\": suspicious}\n\n    # --- Chat Parsing ---\n    def _parse_chat_for_messages(self, chat_file_path, user_filter=\"\"):\n        msgs, pattern = [], re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(?:[+]\\d{2}:\\d{2})?) - (.*?): (.*)')\n        with open(chat_file_path,\"r\",encoding=\"utf-8\") as f:\n            for line in f:\n                m = pattern.match(line.strip())\n                if m:\n                    ts, user, msg = m.groups()\n                    if user_filter==\"\" or user_filter.lower() in user.lower():\n                        msgs.append({\"date\":ts,\"user\":user,\"message\":msg})\n        return msgs\n\n    # --- Bot Detection ---\n    def _detect_bot_behavior(self, file_path):\n        with open(file_path,\"r\",encoding=\"utf-8\") as f: lines=f.readlines()\n        users=[re.match(r\".*?-\\s(.*?):\",l).group(1).strip() for l in lines if re.match(r\".*?-\\s(.*?):\",l)]\n        if not users: return {\"error\":\"No users found.\"}\n        counts=Counter(users)\n        return {\"detected_admin\":counts.most_common(1)[0][0], \"message_counts\":dict(counts)}\n\n    # --- BSE Bulk Deals ---\n    def _get_bse_bulk_deals(self):\n        try:\n            url=\"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers={\"User-Agent\":\"Mozilla/5.0\"}\n            resp=requests.get(url, headers=headers); resp.raise_for_status()\n            for table in pd.read_html(StringIO(resp.text)):\n                if \"Client Name\" in table.columns:\n                    table.columns=table.columns.str.strip()\n                    table[\"Deal Date\"]=pd.to_datetime(table[\"Deal Date\"], format=\"%d/%m/%Y\")\n                    return table\n        except: return pd.DataFrame()\n        return pd.DataFrame()\n\n    # --- Insider Trading ---\n    def _check_insider_trading(self, scam_messages, insider_df, lookback_days=7):\n        if insider_df.empty: return [\"No insider red flags detected.\"]\n        alerts=[]\n        for msg in scam_messages:\n            stock_info = msg.get(\"analysis\",{}).get(\"stock_mention_analysis\")\n            if not stock_info: continue\n            for stock in stock_info[\"mentioned_stocks\"]:\n                if 'SYMBOL' in insider_df.columns:\n                    relevant = insider_df[\n                        (insider_df['SYMBOL'].str.upper()==stock.upper()) &\n                        (pd.to_datetime(insider_df['DATE OF ALLOTMENT/ACQUISITION FROM']) >= pd.Timestamp.now()-pd.Timedelta(days=lookback_days))\n                    ]\n                    for _,trade in relevant.iterrows():\n                        alerts.append(f\"🚨 Insider Alert: {trade.get('NAME OF THE ACQUIRER/DISPOSER','Unknown')} ({trade.get('CATEGORY OF PERSON','')}) traded {trade.get('SYMBOL','')} ({trade.get('ACQUISITION/DISPOSAL TRANSACTION TYPE','')}) on {pd.to_datetime(trade['DATE OF ALLOTMENT/ACQUISITION FROM']).date()} while scam messages mentioning this stock were circulating.\")\n        return alerts if alerts else [\"No insider red flags detected.\"]\n\n    # --- Sentiment vs Price ---\n    def _sentiment_price_check(self, scam_messages):\n        analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n        alerts=[]\n        all_stocks=set()\n        for msg in scam_messages:\n            si=msg.get(\"analysis\",{}).get(\"stock_mention_analysis\")\n            if si: all_stocks.update(si[\"mentioned_stocks\"])\n        if not all_stocks: return [\"No suspicious sentiment/price mismatches.\"]\n        tickers=[s+\".NS\" for s in all_stocks]\n        try: price_data=yf.download(tickers, period=\"5d\", interval=\"1d\", group_by='ticker', progress=False)\n        except: price_data=pd.DataFrame()\n        for msg in scam_messages:\n            si=msg.get(\"analysis\",{}).get(\"stock_mention_analysis\")\n            if not si: continue\n            sentiment = analyzer(msg[\"message\"][:250])[0][\"label\"].lower()\n            for stock in si[\"mentioned_stocks\"]:\n                try:\n                    if stock+\".NS\" in price_data:\n                        df=price_data[stock+\".NS\"]\n                        if not df.empty and len(df[\"Close\"])>1:\n                            change = df[\"Close\"].iloc[-1]-df[\"Close\"].iloc[-2]\n                            if \"positive\" in sentiment and change<0: alerts.append(f\"🚨 Positive sentiment for {stock} but price fell → suspicious hype.\")\n                            if \"negative\" in sentiment and change>0: alerts.append(f\"🚨 Negative sentiment for {stock} but price rose → suspicious suppression.\")\n                except: continue\n        return alerts if alerts else [\"No suspicious sentiment/price mismatches.\"]\n\n    # --- Run Pipeline ---\n    def run(self, chat_file_path, image_folder_path, sebi_id_to_check, entity_name_to_track, insider_csv_path=None):\n        print(\"🚀 Starting Full Analysis Pipeline...\")\n        final_report = defaultdict(dict)\n        final_report[\"manual_verifications\"] = {}\n\n        # Chat & market\n        final_report[\"manual_verifications\"][\"sebi_id_analysis\"]=self._verify_sebi_id(sebi_id_to_check)\n        bulk_df=self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"]={\"trades_found_today\":len(bulk_df)}\n\n        # Messages\n        msgs=self._parse_chat_for_messages(chat_file_path)\n        final_report[\"message_by_message_analysis\"]=[]\n        scam_preds=[]\n        for msg in msgs:\n            text=msg[\"message\"]\n            analysis={\"text_scam_prediction\": self.model.predict(self.vectorizer.transform([text]))[0]}\n            scam_preds.append(analysis[\"text_scam_prediction\"])\n            urls=re.findall(r\"(https?://\\S+)\", text)\n            if urls: analysis[\"url_analysis\"]=[self._check_url_risk(u) for u in urls]\n            stock_analysis=self._analyze_stock_mentions(text)\n            if stock_analysis: analysis[\"stock_mention_analysis\"]=stock_analysis\n            final_report[\"message_by_message_analysis\"].append({\"date\":msg[\"date\"],\"user\":msg[\"user\"],\"message\":text,\"analysis\":analysis})\n\n        # Insider trading\n        insider_df=pd.read_csv(insider_csv_path) if insider_csv_path and os.path.exists(insider_csv_path) else pd.DataFrame()\n        required_cols=['SYMBOL','DATE OF ALLOTMENT/ACQUISITION FROM','NAME OF THE ACQUIRER/DISPOSER','CATEGORY OF PERSON','ACQUISITION/DISPOSAL TRANSACTION TYPE']\n        for col in required_cols:\n            if col not in insider_df.columns: insider_df[col]=None\n        final_report[\"insider_trading_flags\"]=self._check_insider_trading(final_report[\"message_by_message_analysis\"], insider_df)\n\n        # Sentiment\n        final_report[\"sentiment_price_flags\"]=self._sentiment_price_check(final_report[\"message_by_message_analysis\"])\n\n        print(\"\\n✅ Pipeline finished successfully!\")\n        return final_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:20:51.952301Z","iopub.execute_input":"2025-09-20T13:20:51.953812Z","iopub.status.idle":"2025-09-20T13:20:51.989578Z","shell.execute_reply.started":"2025-09-20T13:20:51.953783Z","shell.execute_reply":"2025-09-20T13:20:51.988849Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import os\nimport re\nimport glob\nimport joblib\nimport datetime\nimport pandas as pd\nfrom collections import defaultdict, Counter\nfrom urllib.parse import urlparse\nimport whois\nimport cv2\nimport easyocr\nimport requests\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import pipeline\nimport yfinance as yf\n\nclass FraudDetectionPipeline:\n    \"\"\"\n    Enhanced multi-modal fraud detection pipeline with:\n    - Chat analysis\n    - Stock mention detection\n    - Stock prediction verification (optimized)\n    - URL and QR risk detection\n    - Image OCR analysis\n    - Insider trading alerts\n    - Sentiment vs price check\n    \"\"\"\n\n    def __init__(self, model_path, vectorizer_path, stock_csv_path):\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False)\n            print(\"✅ Pipeline initialized successfully.\")\n        except Exception as e:\n            raise IOError(f\"❌ Failed to load models. Error: {e}\")\n\n        # Load stock symbols and company mapping\n        stock_df = pd.read_csv(stock_csv_path)\n        self.ticker_set = set(stock_df[\"SYMBOL\"].str.upper().tolist())\n        self.company_name_map = dict(zip(\n            stock_df[\"NAME OF COMPANY\"].str.lower(),\n            stock_df[\"SYMBOL\"].str.upper()\n        ))\n\n    # ------------------ URL Risk Analysis ------------------\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"):\n                score += 30\n                reasons.append(\"URL is not secure (HTTP)\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain):\n                score += 40\n                reasons.append(\"URL uses an IP address\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]):\n                score += 25\n                reasons.append(\"URL uses a suspicious TLD\")\n            try:\n                domain_info = whois.whois(domain)\n                creation_date = domain_info.creation_date\n                if creation_date:\n                    creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                    age_days = (datetime.datetime.now() - creation_date).days\n                    if age_days < 180:\n                        score += 30\n                        reasons.append(f\"Domain is very new ({age_days} days old)\")\n            except Exception:\n                score += 10\n                reasons.append(\"WHOIS lookup failed\")\n        except Exception:\n            return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL is malformed\"]}\n        risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n    # ------------------ QR Code Analysis ------------------\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None:\n                return None\n            qr_detector = cv2.QRCodeDetector()\n            data, _, _ = qr_detector.detectAndDecode(img)\n            if data:\n                return {\n                    \"qr_data\": data,\n                    \"analysis\": self._check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]}\n                }\n        except Exception:\n            return None\n        return None\n\n    # ------------------ Image Content Analysis ------------------\n    def _analyze_image_content(self, image_path):\n        results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": self._check_qr_code(image_path)}\n        try:\n            extracted_text = \" \".join(self.ocr_reader.readtext(image_path, detail=0, paragraph=True))\n            if extracted_text.strip():\n                results[\"ocr_text\"] = extracted_text\n                vec = self.vectorizer.transform([extracted_text])\n                results[\"text_scam_prediction\"] = self.model.predict(vec)[0]\n        except Exception:\n            pass\n        return results\n\n    # ------------------ SEBI ID Verification ------------------\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id or not sebi_id.strip():\n            return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n        return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\n    # ------------------ Stock Detection in Text ------------------\n    def detect_stocks_in_text(self, text):\n        found_stocks = set()\n        text_lower = text.lower()\n        for name, symbol in self.company_name_map.items():\n            if name in text_lower:\n                found_stocks.add(symbol)\n        words = re.findall(r'\\b\\w+\\b', text)\n        for word in words:\n            if word.upper() in self.ticker_set:\n                found_stocks.add(word.upper())\n        return list(found_stocks)\n\n    def _analyze_stock_mentions(self, text):\n        detected_stocks = self.detect_stocks_in_text(text)\n        if not detected_stocks:\n            return None\n        suspicious_patterns = []\n        for kw in [\"guaranteed\", \"10x\", \"insider\"]:\n            if kw.lower() in text.lower():\n                suspicious_patterns.append(\"Message contains pump-and-dump keywords.\")\n                break\n        return {\"mentioned_stocks\": detected_stocks, \"suspicious_patterns\": suspicious_patterns}\n\n    # ------------------ Stock Prediction Verification ------------------\n    def _verify_stock_predictions_batch(self, messages, horizon_days=3):\n        \"\"\"\n        Batch verification for all stocks mentioned in messages.\n        \"\"\"\n        # 1. Collect all unique stocks\n        all_stocks = set()\n        for msg in messages:\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if stock_info:\n                all_stocks.update(stock_info[\"mentioned_stocks\"])\n\n        if not all_stocks:\n            return {}\n\n        # 2. Fetch Yahoo Finance data in one go per stock\n        yf_data = {}\n        for stock in all_stocks:\n            try:\n                ticker = stock + \".NS\"\n                min_date = min(pd.to_datetime(msg[\"date\"]) for msg in messages)\n                max_date = max(pd.to_datetime(msg[\"date\"]) + pd.Timedelta(days=horizon_days) for msg in messages)\n                yf_data[stock] = yf.download(ticker, start=min_date.date(), end=max_date.date(), progress=False)\n            except Exception:\n                yf_data[stock] = pd.DataFrame()\n\n        # 3. Verify each message prediction\n        # 3. Verify each message prediction\n        results = {}\n        for msg in messages:\n            text = msg[\"message\"]\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not stock_info:\n                continue\n        \n            verifications = []\n            for stock in stock_info[\"mentioned_stocks\"]:\n                df = yf_data.get(stock)\n                if df is None or df.empty:\n                    continue\n        \n                # Handle multi-index or single ticker DataFrame\n                if isinstance(df.columns, pd.MultiIndex):\n                    # Multi-ticker DataFrame: pick the 'Close' column for the specific stock\n                    try:\n                        close_prices = df[\"Close\"][stock + \".NS\"]\n                    except KeyError:\n                        continue\n                else:\n                    # Single-ticker DataFrame\n                    close_prices = df[\"Close\"]\n        \n                if len(close_prices) > 1:\n                    change = close_prices.iloc[-1] - close_prices.iloc[0]\n                    actual_direction = \"UP\" if change > 0 else \"DOWN\"\n                    predicted_up = bool(re.search(r'\\b(up|increase|gain|rally|soar|\\+)\\b', text, re.IGNORECASE))\n                    predicted_down = bool(re.search(r'\\b(down|fall|drop|loss|-)\\b', text, re.IGNORECASE))\n                    match = (predicted_up and change > 0) or (predicted_down and change < 0)\n        \n                    verifications.append({\n                        \"stock\": stock,\n                        \"predicted_text\": text,\n                        \"actual_change\": change,\n                        \"actual_direction\": actual_direction,\n                        \"prediction_correct\": match\n                    })\n        \n            if verifications:\n                results[msg[\"date\"]] = verifications\n        \n        return results\n\n\n    # ------------------ Chat Parsing ------------------\n    def _parse_chat_for_messages(self, chat_file_path, user_filter=\"\"):\n        messages = []\n        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(?:[+]\\d{2}:\\d{2})?) - (.*?): (.*)')\n        with open(chat_file_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                match = pattern.match(line.strip())\n                if match:\n                    timestamp, user, message = match.groups()\n                    if user_filter == \"\" or user_filter.lower() in user.lower():\n                        messages.append({\"date\": timestamp, \"user\": user, \"message\": message})\n        return messages\n\n    # ------------------ BSE Bulk Deals ------------------\n    def _get_bse_bulk_deals(self):\n        try:\n            url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            for table in pd.read_html(StringIO(response.text)):\n                if \"Client Name\" in table.columns:\n                    table.columns = table.columns.str.strip()\n                    table[\"Deal Date\"] = pd.to_datetime(table[\"Deal Date\"], format=\"%d/%m/%Y\")\n                    return table\n        except Exception:\n            return pd.DataFrame()\n        return pd.DataFrame()\n\n    # ------------------ Insider Trading ------------------\n    def _check_insider_trading(self, scam_messages, insider_df, lookback_days=7):\n        alerts = []\n        for msg in scam_messages:\n            text = msg['message']\n            stock_info = msg.get('analysis', {}).get('stock_mention_analysis')\n            if not stock_info:\n                continue\n            for stock in stock_info['mentioned_stocks']:\n                relevant_trades = insider_df[\n                    (insider_df['SYMBOL'].str.upper() == stock.upper()) &\n                    (pd.to_datetime(insider_df['DATE OF ALLOTMENT/ACQUISITION FROM']) >= pd.Timestamp.now() - pd.Timedelta(days=lookback_days))\n                ]\n                for _, trade in relevant_trades.iterrows():\n                    alerts.append(\n                        f\"🚨 Insider Alert: {trade['NAME OF THE ACQUIRER/DISPOSER']} \"\n                        f\"({trade['CATEGORY OF PERSON']}) traded {trade['SYMBOL']} \"\n                        f\"({trade['ACQUISITION/DISPOSAL TRANSACTION TYPE']}) on \"\n                        f\"{pd.to_datetime(trade['DATE OF ALLOTMENT/ACQUISITION FROM']).date()} \"\n                        f\"while scam messages mentioning this stock were circulating.\"\n                    )\n        return alerts if alerts else [\"No insider red flags detected.\"]\n\n    # ------------------ Sentiment vs Price ------------------\n    def _sentiment_price_check(self, scam_messages):\n        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n        alerts = []\n        all_stocks = set()\n        for msg in scam_messages:\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if stock_info:\n                all_stocks.update(stock_info[\"mentioned_stocks\"])\n        if not all_stocks:\n            return [\"No suspicious sentiment/price mismatches.\"]\n        tickers = [s + \".NS\" for s in all_stocks]\n        try:\n            price_data = yf.download(tickers, period=\"5d\", interval=\"1d\", group_by='ticker', progress=False)\n        except Exception:\n            price_data = pd.DataFrame()\n        for msg in scam_messages:\n            text = msg[\"message\"]\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not stock_info:\n                continue\n            sentiment = sentiment_analyzer(text[:250])[0]\n            label = sentiment[\"label\"].lower()\n            for stock in stock_info[\"mentioned_stocks\"]:\n                try:\n                    if stock + \".NS\" in price_data:\n                        df = price_data[stock + \".NS\"]\n                        if not df.empty and len(df[\"Close\"]) > 1:\n                            change = df[\"Close\"].iloc[-1] - df[\"Close\"].iloc[-2]\n                            if \"positive\" in label and change < 0:\n                                alerts.append(f\"🚨 Positive sentiment for {stock} but price fell → suspicious hype.\")\n                            if \"negative\" in label and change > 0:\n                                alerts.append(f\"🚨 Negative sentiment for {stock} but price rose → suspicious suppression.\")\n                except Exception:\n                    continue\n        return alerts if alerts else [\"No suspicious sentiment/price mismatches.\"]\n\n    # ------------------ Full Pipeline ------------------\n    def run(self, chat_file_path, image_folder_path, sebi_id_to_check, insider_csv_path=None):\n        print(\"🚀 Starting Full Analysis Pipeline...\")\n        final_report = defaultdict(dict)\n        final_report[\"manual_verifications\"] = {}\n\n        # SEBI verification\n        final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = self._verify_sebi_id(sebi_id_to_check)\n\n        # Bulk deals\n        bulk_deals_df = self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n\n        # Messages\n        all_messages = self._parse_chat_for_messages(chat_file_path)\n        final_report[\"message_by_message_analysis\"] = []\n\n        for msg_data in all_messages:\n            text = msg_data[\"message\"]\n            analysis = {\"text_scam_prediction\": self.model.predict(self.vectorizer.transform([text]))[0]}\n\n            # URL analysis\n            urls = re.findall(r\"(https?://\\S+)\", text)\n            if urls:\n                analysis[\"url_analysis\"] = [self._check_url_risk(url) for url in urls]\n\n            # Stock mentions\n            stock_analysis = self._analyze_stock_mentions(text)\n            if stock_analysis:\n                analysis[\"stock_mention_analysis\"] = stock_analysis\n\n            final_report[\"message_by_message_analysis\"].append({\n                \"date\": msg_data[\"date\"],\n                \"user\": msg_data[\"user\"],\n                \"message\": text,\n                \"analysis\": analysis\n            })\n\n        # Stock prediction verification (batched)\n        predictions = self._verify_stock_predictions_batch(final_report[\"message_by_message_analysis\"])\n        for msg in final_report[\"message_by_message_analysis\"]:\n            msg[\"analysis\"][\"stock_prediction_verification\"] = predictions.get(msg[\"date\"], [])\n\n        # Insider trading\n        if insider_csv_path and os.path.exists(insider_csv_path):\n            insider_df = pd.read_csv(insider_csv_path)\n        else:\n            insider_df = pd.DataFrame()\n        required_cols = ['SYMBOL', 'DATE OF ALLOTMENT/ACQUISITION FROM', 'NAME OF THE ACQUIRER/DISPOSER',\n                         'CATEGORY OF PERSON', 'ACQUISITION/DISPOSAL TRANSACTION TYPE']\n        for col in required_cols:\n            if col not in insider_df.columns:\n                insider_df[col] = None\n        final_report[\"insider_trading_flags\"] = self._check_insider_trading(final_report[\"message_by_message_analysis\"], insider_df)\n\n        # Sentiment vs price\n        final_report[\"sentiment_price_flags\"] = self._sentiment_price_check(final_report[\"message_by_message_analysis\"])\n\n        # Image OCR + QR analysis\n        image_files = glob.glob(os.path.join(image_folder_path, \"*\"))\n        final_report[\"image_analysis\"] = []\n        for img_path in image_files:\n            img_report = self._analyze_image_content(img_path)\n            final_report[\"image_analysis\"].append({\n                \"file_name\": os.path.basename(img_path),\n                \"analysis\": img_report\n            })\n\n        print(\"✅ Pipeline finished successfully!\")\n        return final_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T15:03:03.123327Z","iopub.execute_input":"2025-09-20T15:03:03.124004Z","iopub.status.idle":"2025-09-20T15:03:03.161968Z","shell.execute_reply.started":"2025-09-20T15:03:03.123965Z","shell.execute_reply":"2025-09-20T15:03:03.161392Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\nSTOCK_PATH = \"/kaggle/input/nse-stocks/EQUITY_L.csv\"\n\n# Input data paths\nchat_file = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/chat/Extracted messages....txt\"\nimage_folder = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/images\"\n\n# Entity information\nsebi_id_input = \"INZ000048660\"\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\n\ndef print_pipeline_report(report, max_messages=5):\n    print(\"\\n\" + \"=\"*70)\n    print(\"🚨 FRAUD DETECTION & STOCK VERIFICATION PIPELINE REPORT\")\n    print(\"=\"*70 + \"\\n\")\n\n    # --- SEBI ID Verification ---\n    sebi_report = report.get(\"manual_verifications\", {}).get(\"sebi_id_analysis\", {})\n    print(\"🔹 SEBI ID Verification:\")\n    print(f\"   SEBI ID: {sebi_report.get('sebi_id', 'N/A')}\")\n    print(f\"   Status: {sebi_report.get('status', 'N/A')}\\n\")\n\n    # --- Bulk Deals ---\n    bulk_deals_count = report.get(\"bulk_deal_analysis\", {}).get(\"trades_found_today\", 0)\n    print(f\"🔹 BSE/NSE Bulk Deals Found Today: {bulk_deals_count}\\n\")\n\n    # --- Message by Message Analysis ---\n    print(\"🔹 Chat Messages Analysis:\")\n    messages = report.get(\"message_by_message_analysis\", [])\n    for msg in messages[:max_messages]:\n        print(f\"  [{msg['date']}] {msg['user']}: {msg['message']}\")\n        analysis = msg.get(\"analysis\", {})\n\n        # Scam Prediction\n        print(f\"     - Scam Prediction: {analysis.get('text_scam_prediction', 'N/A')}\")\n\n        # URL analysis\n        if \"url_analysis\" in analysis:\n            for url_res in analysis[\"url_analysis\"]:\n                print(f\"     - URL Risk: {url_res['url']} -> {url_res['risk_level']} ({url_res['risk_score']}%)\")\n                if url_res.get(\"reasons\"):\n                    print(f\"       Reasons: {', '.join(url_res['reasons'])}\")\n\n        # Stock mentions\n        if \"stock_mention_analysis\" in analysis:\n            stocks = analysis[\"stock_mention_analysis\"].get(\"mentioned_stocks\", [])\n            print(f\"     - Mentioned Stocks: {', '.join(stocks) if stocks else 'None'}\")\n            if analysis[\"stock_mention_analysis\"].get(\"suspicious_patterns\"):\n                print(f\"       Suspicious Patterns: {', '.join(analysis['stock_mention_analysis']['suspicious_patterns'])}\")\n\n        # Stock prediction verification\n        if \"stock_prediction_verification\" in analysis and analysis[\"stock_prediction_verification\"]:\n            print(f\"     - Stock Prediction Verification:\")\n            for ver in analysis[\"stock_prediction_verification\"]:\n                # Extract predicted direction from text\n                predicted_up = bool(re.search(r'\\b(up|increase|gain|rally|soar|\\+)\\b', ver['predicted_text'], re.IGNORECASE))\n                predicted_down = bool(re.search(r'\\b(down|fall|drop|loss|-)\\b', ver['predicted_text'], re.IGNORECASE))\n                predicted_direction = \"UP\" if predicted_up else \"DOWN\" if predicted_down else \"N/A\"\n        \n                print(f\"       * {ver['stock']}: Predicted: {predicted_direction}, \"\n                      f\"Actual: {ver['actual_direction']} ({ver['actual_change']:.2f}) -> Correct: {ver['prediction_correct']}\")\n        print(\"\")\n\n    if len(messages) > max_messages:\n        print(f\"  ...and {len(messages) - max_messages} more messages.\\n\")\n\n    # --- Insider Trading Flags ---\n    print(\"🔹 Insider Trading Alerts:\")\n    insider_alerts = report.get(\"insider_trading_flags\", [])\n    if insider_alerts:\n        for alert in insider_alerts:\n            print(f\"   - {alert}\")\n    else:\n        print(\"   - None\")\n    print(\"\")\n\n    # --- Sentiment vs Price Flags ---\n    print(\"🔹 Sentiment vs Price Mismatches:\")\n    sentiment_alerts = report.get(\"sentiment_price_flags\", [])\n    if sentiment_alerts:\n        for alert in sentiment_alerts:\n            print(f\"   - {alert}\")\n    else:\n        print(\"   - None\")\n    print(\"\")\n\n    # --- Image Analysis ---\n    print(\"🔹 Image OCR & QR Analysis:\")\n    for img in report.get(\"image_analysis\", []):\n        print(f\"   - {img['file_name']}:\")\n        print(f\"       OCR Text Length: {len(img['analysis'].get('ocr_text', ''))}\")\n        print(f\"       Text Scam Prediction: {img['analysis'].get('text_scam_prediction', 'N/A')}\")\n        qr = img['analysis'].get(\"qr_analysis\")\n        if qr:\n            print(f\"       QR Risk: {qr.get('risk_level', 'N/A')}\")\n            if qr.get(\"reasons\"):\n                print(f\"       QR Reasons: {', '.join(qr['reasons'])}\")\n        print(\"\")\n\n    # --- Summary ---\n    print(\"=\"*70)\n    print(\"✅ End of Report\")\n    print(\"=\"*70 + \"\\n\")\n\n\n# Main execution\ntry:\n    fraud_detector = FraudDetectionPipeline(\n        model_path=MODEL_PATH, \n        vectorizer_path=VECTORIZER_PATH, \n        stock_csv_path=STOCK_PATH\n    )\n    \n    if os.path.exists(chat_file):\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=sebi_id_input,\n        )\n        print_pipeline_report(results, max_messages=10)\n    else:\n        print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")\n\nexcept IOError as e:\n    print(e)\n    print(\"Please run the 'train_scam_classifier()' function first to generate model files.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport glob\nimport joblib\nimport datetime\nimport pandas as pd\nfrom io import StringIO\nfrom collections import defaultdict, Counter\nfrom urllib.parse import urlparse\nimport whois\nimport cv2\nimport easyocr\nimport requests\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import pipeline\nimport yfinance as yf\n\nclass FraudDetectionPipeline:\n    \"\"\"\n    Enhanced multi-modal fraud detection pipeline with:\n    - Chat analysis\n    - Stock mention detection\n    - Stock prediction verification (optimized)\n    - URL and QR risk detection\n    - Image OCR analysis\n    - Insider trading alerts\n    - Sentiment vs price check\n    - User prediction accuracy evaluation\n    \"\"\"\n\n    def __init__(self, model_path, vectorizer_path, stock_csv_path):\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False)\n            print(\"✅ Pipeline initialized successfully.\")\n        except Exception as e:\n            raise IOError(f\"❌ Failed to load models. Error: {e}\")\n\n        # Load stock symbols and company mapping\n        stock_df = pd.read_csv(stock_csv_path)\n        self.ticker_set = set(stock_df[\"SYMBOL\"].str.upper().tolist())\n        self.company_name_map = dict(zip(\n            stock_df[\"NAME OF COMPANY\"].str.lower(),\n            stock_df[\"SYMBOL\"].str.upper()\n        ))\n\n    # ------------------ URL Risk Analysis ------------------\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"):\n                score += 30\n                reasons.append(\"URL is not secure (HTTP)\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain):\n                score += 40\n                reasons.append(\"URL uses an IP address\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]):\n                score += 25\n                reasons.append(\"URL uses a suspicious TLD\")\n            try:\n                domain_info = whois.whois(domain)\n                creation_date = domain_info.creation_date\n                if creation_date:\n                    creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                    age_days = (datetime.datetime.now() - creation_date).days\n                    if age_days < 180:\n                        score += 30\n                        reasons.append(f\"Domain is very new ({age_days} days old)\")\n            except Exception:\n                score += 10\n                reasons.append(\"WHOIS lookup failed\")\n        except Exception:\n            return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL is malformed\"]}\n        risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n    # ------------------ QR Code Analysis ------------------\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None:\n                return None\n            qr_detector = cv2.QRCodeDetector()\n            data, _, _ = qr_detector.detectAndDecode(img)\n            if data:\n                return {\n                    \"qr_data\": data,\n                    \"analysis\": self._check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]}\n                }\n        except Exception:\n            return None\n        return None\n\n    # ------------------ Image Content Analysis ------------------\n    def _analyze_image_content(self, image_path):\n        results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": self._check_qr_code(image_path)}\n        try:\n            extracted_text = \" \".join(self.ocr_reader.readtext(image_path, detail=0, paragraph=True))\n            if extracted_text.strip():\n                results[\"ocr_text\"] = extracted_text\n                vec = self.vectorizer.transform([extracted_text])\n                # --- FIX --- Cast to standard string\n                prediction = self.model.predict(vec)[0]\n                results[\"text_scam_prediction\"] = str(prediction)\n        except Exception:\n            pass\n        return results\n\n    # ------------------ SEBI ID Verification ------------------\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id or not sebi_id.strip():\n            return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n        return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\n    # ------------------ Stock Detection in Text ------------------\n    def detect_stocks_in_text(self, text):\n        found_stocks = set()\n        text_lower = text.lower()\n        for name, symbol in self.company_name_map.items():\n            if name in text_lower:\n                found_stocks.add(symbol)\n        words = re.findall(r'\\b\\w+\\b', text)\n        for word in words:\n            if word.upper() in self.ticker_set:\n                found_stocks.add(word.upper())\n        return list(found_stocks)\n\n    def _analyze_stock_mentions(self, text):\n        detected_stocks = self.detect_stocks_in_text(text)\n        if not detected_stocks:\n            return None\n        suspicious_patterns = []\n        for kw in [\"guaranteed\", \"10x\", \"insider\"]:\n            if kw.lower() in text.lower():\n                suspicious_patterns.append(\"Message contains pump-and-dump keywords.\")\n                break\n        return {\"mentioned_stocks\": detected_stocks, \"suspicious_patterns\": suspicious_patterns}\n\n    # ------------------ Stock Prediction Verification ------------------\n    # ------------------ Stock Prediction Verification ------------------\n    def _verify_stock_predictions_batch(self, messages, horizon_days=3):\n        \"\"\"\n        Batch verification for all stocks mentioned in messages.\n        \"\"\"\n        all_stocks = set()\n        for msg in messages:\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if stock_info:\n                all_stocks.update(stock_info[\"mentioned_stocks\"])\n        if not all_stocks:\n            return {}\n\n        yf_data = {}\n        for stock in all_stocks:\n            try:\n                ticker = stock + \".NS\"\n                min_date = min(pd.to_datetime(msg[\"date\"]) for msg in messages)\n                max_date = max(pd.to_datetime(msg[\"date\"]) + pd.Timedelta(days=horizon_days) for msg in messages)\n                yf_data[stock] = yf.download(ticker, start=min_date.date(), end=max_date.date(), progress=False)\n            except Exception:\n                yf_data[stock] = pd.DataFrame()\n\n        results = {}\n        for msg in messages:\n            text = msg[\"message\"]\n            msg_date = msg[\"date\"]\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not stock_info:\n                continue\n\n            verifications = []\n            for stock in stock_info[\"mentioned_stocks\"]:\n                df = yf_data.get(stock)\n                if df is None or df.empty:\n                    continue\n                \n                start_date = pd.to_datetime(msg_date).tz_localize(None)\n                end_date = start_date + pd.Timedelta(days=horizon_days)\n                relevant_df = df[(df.index >= start_date) & (df.index <= end_date)]\n\n                if len(relevant_df) > 1:\n                    change = relevant_df[\"Close\"].iloc[-1] - relevant_df[\"Close\"].iloc[0]\n                    actual_direction = \"UP\" if (change > 0).any() else \"DOWN\"\n                    \n                    predicted_up = bool(re.search(r'\\b(up|increase|gain|rally|soar|\\+)\\b', text, re.IGNORECASE))\n                    predicted_down = bool(re.search(r'\\b(down|fall|drop|loss|-)\\b', text, re.IGNORECASE))\n                    \n                    went_up = (change > 0)\n                    if isinstance(went_up, pd.Series):\n                        went_up = went_up.any()\n\n                    went_down = (change < 0)\n                    if isinstance(went_down, pd.Series):\n                        went_down = went_down.any()\n\n                    match = (predicted_up and went_up) or (predicted_down and went_down)\n\n                    verifications.append({\n                        \"stock\": stock,\n                        \"predicted_text\": text,\n                        \"actual_change\": float(change.item() if isinstance(change, pd.Series) else change),\n                        \"actual_direction\": actual_direction,\n                        # --- FIX IS HERE ---\n                        # Explicitly cast the 'match' variable to a standard Python bool\n                        \"prediction_correct\": bool(match)\n                    })\n            if verifications:\n                results[msg_date] = verifications\n        return results\n\n    # ------------------ Chat Parsing ------------------\n    def _parse_chat_for_messages(self, chat_file_path, user_filter=\"\"):\n        messages = []\n        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(?:[+]\\d{2}:\\d{2})?) - (.*?): (.*)')\n        with open(chat_file_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                match = pattern.match(line.strip())\n                if match:\n                    timestamp, user, message = match.groups()\n                    if user_filter == \"\" or user_filter.lower() in user.lower():\n                        messages.append({\"date\": timestamp, \"user\": user, \"message\": message})\n        return messages\n\n    # ------------------ BSE Bulk Deals ------------------\n    def _get_bse_bulk_deals(self):\n        try:\n            url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            for table in pd.read_html(StringIO(response.text)):\n                if \"Client Name\" in table.columns:\n                    table.columns = table.columns.str.strip()\n                    table[\"Deal Date\"] = pd.to_datetime(table[\"Deal Date\"], format=\"%d/%m/%Y\")\n                    return table\n        except Exception:\n            return pd.DataFrame()\n        return pd.DataFrame()\n\n    # ------------------ Insider Trading ------------------\n    def _check_insider_trading(self, scam_messages, insider_df, lookback_days=7):\n        alerts = []\n        for msg in scam_messages:\n            text = msg['message']\n            stock_info = msg.get('analysis', {}).get('stock_mention_analysis')\n            if not stock_info:\n                continue\n            for stock in stock_info['mentioned_stocks']:\n                relevant_trades = insider_df[\n                    (insider_df['SYMBOL'].str.upper() == stock.upper()) &\n                    (pd.to_datetime(insider_df['DATE OF ALLOTMENT/ACQUISITION FROM']) >= pd.Timestamp.now() - pd.Timedelta(days=lookback_days))\n                ]\n                for _, trade in relevant_trades.iterrows():\n                    alerts.append(\n                        f\"🚨 Insider Alert: {trade['NAME OF THE ACQUIRER/DISPOSER']} \"\n                        f\"({trade['CATEGORY OF PERSON']}) traded {trade['SYMBOL']} \"\n                        f\"({trade['ACQUISITION/DISPOSAL TRANSACTION TYPE']}) on \"\n                        f\"{pd.to_datetime(trade['DATE OF ALLOTMENT/ACQUISITION FROM']).date()} \"\n                        f\"while scam messages mentioning this stock were circulating.\"\n                    )\n        return alerts if alerts else [\"No insider red flags detected.\"]\n\n    # ------------------ Sentiment vs Price ------------------\n    # ------------------ Sentiment vs Price ------------------\n    def _sentiment_price_check(self, scam_messages):\n        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n        alerts = []\n        all_stocks = set()\n        for msg in scam_messages:\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if stock_info:\n                all_stocks.update(stock_info[\"mentioned_stocks\"])\n        \n        if not all_stocks:\n            return [\"No suspicious sentiment/price mismatches.\"]\n        \n        tickers = [s + \".NS\" for s in all_stocks]\n        try:\n            price_data = yf.download(tickers, period=\"5d\", interval=\"1d\", group_by='ticker', progress=False)\n        except Exception:\n            return [\"Could not fetch price data for sentiment check.\"]\n\n        # --- FIX IS HERE ---\n        # Handle the two cases for yfinance's returned DataFrame structure\n        is_multi_index = isinstance(price_data.columns, pd.MultiIndex)\n\n        for msg in scam_messages:\n            text = msg[\"message\"]\n            stock_info = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not stock_info:\n                continue\n            \n            sentiment = sentiment_analyzer(text[:250])[0]\n            label = sentiment[\"label\"].lower()\n            \n            for stock in stock_info[\"mentioned_stocks\"]:\n                try:\n                    df = pd.DataFrame() # Start with an empty DataFrame\n\n                    # Case 1: Multiple stocks were downloaded successfully\n                    if is_multi_index:\n                        # Check if this stock's data is present before trying to access it\n                        if (stock + \".NS\") in price_data.columns.get_level_values(1):\n                            # Correctly select the data for one ticker from the multi-index\n                            df = price_data.xs(stock + \".NS\", level=1, axis=1)\n                    \n                    # Case 2: Only one stock was downloaded successfully\n                    elif not price_data.empty:\n                        df = price_data\n\n                    # Now, perform the analysis only if we successfully extracted the stock's data\n                    if not df.empty and len(df[\"Close\"]) > 1 and not df[\"Close\"].isnull().all():\n                        change = df[\"Close\"].iloc[-1] - df[\"Close\"].iloc[-2]\n                        if \"positive\" in label and change < 0:\n                            alerts.append(f\"🚨 Positive sentiment for {stock} but price fell → suspicious hype.\")\n                        if \"negative\" in label and change > 0:\n                            alerts.append(f\"🚨 Negative sentiment for {stock} but price rose → suspicious suppression.\")\n                \n                except Exception:\n                    # Catch any other errors for a specific stock and continue\n                    continue\n                    \n        return alerts if alerts else [\"No suspicious sentiment/price mismatches.\"]\n\n    # ------------------ User Prediction Accuracy Evaluation ------------------\n    def _evaluate_user_prediction_accuracy(self, analyzed_messages, min_predictions=3, accuracy_threshold=40.0):\n        \"\"\"\n        Analyzes the prediction accuracy of each user and flags potential scammers.\n        \"\"\"\n        print(\"📊 Evaluating user prediction accuracy...\")\n        user_stats = defaultdict(lambda: {'correct': 0, 'incorrect': 0})\n\n        for msg in analyzed_messages:\n            user = msg[\"user\"]\n            verifications = msg.get(\"analysis\", {}).get(\"stock_prediction_verification\", [])\n            for verification in verifications:\n                if verification[\"prediction_correct\"]:\n                    user_stats[user]['correct'] += 1\n                else:\n                    user_stats[user]['incorrect'] += 1\n        \n        accuracy_report = {}\n        for user, stats in user_stats.items():\n            total_predictions = stats['correct'] + stats['incorrect']\n            if total_predictions == 0:\n                continue\n            accuracy = (stats['correct'] / total_predictions) * 100\n            is_potential_scammer = (total_predictions >= min_predictions and accuracy < accuracy_threshold)\n            accuracy_report[user] = {\n                \"correct_predictions\": stats['correct'],\n                \"incorrect_predictions\": stats['incorrect'],\n                \"total_predictions\": total_predictions,\n                \"accuracy_percent\": round(accuracy, 2),\n                \"is_potential_scammer\": is_potential_scammer,\n                \"reason\": f\"Flagged due to low accuracy ({accuracy:.2f}%) across {total_predictions} predictions.\" if is_potential_scammer else \"Not flagged.\"\n            }\n        print(\"✅ User accuracy evaluation complete.\")\n        return accuracy_report\n\n    # ------------------ Full Pipeline ------------------\n    # ------------------ Full Pipeline ------------------\n    def run(self, chat_file_path, image_folder_path, sebi_id_to_check, insider_csv_path=None):\n        print(\"🚀 Starting Full Analysis Pipeline...\")\n        final_report = defaultdict(dict)\n        final_report[\"manual_verifications\"] = {}\n\n        final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = self._verify_sebi_id(sebi_id_to_check)\n        bulk_deals_df = self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n        \n        all_messages = self._parse_chat_for_messages(chat_file_path)\n        final_report[\"message_by_message_analysis\"] = []\n\n        # Loop through all messages but only add relevant ones to the report\n        for msg_data in all_messages:\n            text = msg_data[\"message\"]\n\n            # First, check if the message contains a URL or a stock to see if it's worth processing\n            urls = re.findall(r\"(https?://\\S+)\", text)\n            stock_analysis = self._analyze_stock_mentions(text)\n\n            # --- FILTERING LOGIC IS HERE ---\n            # Only proceed if the message contains a URL or a stock mention\n            if urls or stock_analysis:\n                \n                # Now perform the rest of the analysis on this relevant message\n                prediction = self.model.predict(self.vectorizer.transform([text]))[0]\n                analysis = {\"text_scam_prediction\": str(prediction)}\n\n                if urls:\n                    analysis[\"url_analysis\"] = [self._check_url_risk(url) for url in urls]\n\n                if stock_analysis:\n                    analysis[\"stock_mention_analysis\"] = stock_analysis\n\n                # Append the fully analyzed, relevant message to the report\n                final_report[\"message_by_message_analysis\"].append({\n                    \"date\": msg_data[\"date\"],\n                    \"user\": msg_data[\"user\"],\n                    \"message\": text,\n                    \"analysis\": analysis\n                })\n\n        # All downstream analysis will now run ONLY on the filtered messages\n        predictions = self._verify_stock_predictions_batch(final_report[\"message_by_message_analysis\"])\n        for msg in final_report[\"message_by_message_analysis\"]:\n            if msg.get(\"date\") in predictions:\n                msg[\"analysis\"][\"stock_prediction_verification\"] = predictions[msg[\"date\"]]\n            else:\n                msg[\"analysis\"][\"stock_prediction_verification\"] = []\n\n        if insider_csv_path and os.path.exists(insider_csv_path):\n            insider_df = pd.read_csv(insider_csv_path)\n        else:\n            insider_df = pd.DataFrame()\n        required_cols = ['SYMBOL', 'DATE OF ALLOTMENT/ACQUISITION FROM', 'NAME OF THE ACQUIRER/DISPOSER', 'CATEGORY OF PERSON', 'ACQUISITION/DISPOSAL TRANSACTION TYPE']\n        for col in required_cols:\n            if col not in insider_df.columns:\n                insider_df[col] = None\n        final_report[\"insider_trading_flags\"] = self._check_insider_trading(final_report[\"message_by_message_analysis\"], insider_df)\n\n        final_report[\"sentiment_price_flags\"] = self._sentiment_price_check(final_report[\"message_by_message_analysis\"])\n\n        # Image analysis remains unchanged and will process all images\n        image_files = glob.glob(os.path.join(image_folder_path, \"*\"))\n        final_report[\"image_analysis\"] = []\n        for img_path in image_files:\n            img_report = self._analyze_image_content(img_path)\n            final_report[\"image_analysis\"].append({\n                \"file_name\": os.path.basename(img_path),\n                \"analysis\": img_report\n            })\n\n        user_accuracy_report = self._evaluate_user_prediction_accuracy(final_report[\"message_by_message_analysis\"])\n        final_report[\"user_prediction_accuracy_report\"] = user_accuracy_report\n\n        print(\"✅ Pipeline finished successfully!\")\n        return final_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T18:00:23.852740Z","iopub.execute_input":"2025-09-20T18:00:23.852991Z","iopub.status.idle":"2025-09-20T18:00:48.741614Z","shell.execute_reply.started":"2025-09-20T18:00:23.852972Z","shell.execute_reply":"2025-09-20T18:00:48.740599Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758391228.866984      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758391228.949491      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport json\nimport re\n\n# Define paths\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\nSTOCK_PATH = \"/kaggle/input/nse-stocks/EQUITY_L.csv\"\n\n# Input data paths\nchat_file = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/chat/Extracted messages....txt\"\nimage_folder = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/images\"\n\n# Entity information\nsebi_id_input = \"INZ000048660\"\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\n\n# 🔹 Function to simplify the report for presentation\ndef simplify_report(report):\n    \"\"\"\n    Returns a simplified version of the pipeline report for presentation/demo.\n    \"\"\"\n    simplified = {\n        \"sebi_id_status\": report.get(\"manual_verifications\", {})\n                                 .get(\"sebi_id_analysis\", {})\n                                 .get(\"status\", \"N/A\"),\n        \"bulk_deals_today\": report.get(\"bulk_deal_analysis\", {})\n                                  .get(\"trades_found_today\", 0),\n        \"flagged_users\": {},\n        \"messages\": [],\n        \"images\": []\n    }\n\n    # Users flagged for poor prediction accuracy\n    user_report = report.get(\"user_prediction_accuracy_report\", {})\n    for user, data in user_report.items():\n        if data.get(\"is_potential_scammer\"):\n            simplified[\"flagged_users\"][user] = {\n                \"accuracy_percent\": data.get(\"accuracy_percent\"),\n                \"correct_predictions\": data.get(\"correct_predictions\"),\n                \"incorrect_predictions\": data.get(\"incorrect_predictions\")\n            }\n\n    # Messages with stock mentions or risky URLs\n    for msg in report.get(\"message_by_message_analysis\", []):\n        analysis = msg.get(\"analysis\", {})\n        if \"stock_mention_analysis\" in analysis or \"url_analysis\" in analysis:\n            simplified[\"messages\"].append({\n                \"date\": msg[\"date\"],\n                \"user\": msg[\"user\"],\n                \"message\": msg[\"message\"][:80] + (\"...\" if len(msg[\"message\"]) > 80 else \"\"),\n                \"scam_prediction\": analysis.get(\"text_scam_prediction\", \"N/A\"),\n                \"stocks\": analysis.get(\"stock_mention_analysis\", {})\n                                   .get(\"mentioned_stocks\", []),\n                \"urls\": [\n                    {\n                        \"url\": u[\"url\"],\n                        \"risk\": u[\"risk_level\"],\n                        \"score\": u[\"risk_score\"]\n                    }\n                    for u in analysis.get(\"url_analysis\", [])\n                ]\n            })\n\n    # Images — only keep filename + scam prediction\n    for img in report.get(\"image_analysis\", []):\n        simplified[\"images\"].append({\n            \"file_name\": img[\"file_name\"],\n            \"text_scam_prediction\": img.get(\"analysis\", {}).get(\"text_scam_prediction\", \"N/A\")\n        })\n\n    return simplified\n\n\n# 🔹 Main execution\ntry:\n    # Assuming FraudDetectionPipeline class is defined in another cell or file\n    fraud_detector = FraudDetectionPipeline(\n        model_path=MODEL_PATH, \n        vectorizer_path=VECTORIZER_PATH, \n        stock_csv_path=STOCK_PATH\n    )\n    \n    if os.path.exists(chat_file):\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=sebi_id_input,\n        )\n        \n        # Print simplified JSON for demo\n        print(\"\\n\" + \"=\"*70)\n        print(\"🚨 SIMPLIFIED PIPELINE REPORT (JSON)\")\n        print(\"=\"*70 + \"\\n\")\n        print(json.dumps(simplify_report(results), indent=4))\n        print(\"\\n\" + \"=\"*70)\n        print(\"✅ End of Report\")\n        print(\"=\"*70 + \"\\n\")\n\n    else:\n        print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")\n\nexcept NameError:\n    print(\"❌ ERROR: `FraudDetectionPipeline` class is not defined. Please ensure its code has been executed.\")\nexcept IOError as e:\n    print(e)\n    print(\"Please make sure the model and vectorizer files exist. You may need to train them first.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T18:27:35.511129Z","iopub.execute_input":"2025-09-20T18:27:35.511995Z"}},"outputs":[{"name":"stdout","text":"🚀 Initializing Fraud Detection Pipeline...\n✅ Pipeline initialized successfully.\n🚀 Starting Full Analysis Pipeline...\n","output_type":"stream"},{"name":"stderr","text":"2025-09-20 18:27:54,460 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n2025-09-20 18:28:04,954 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n2025-09-20 18:28:15,555 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n2025-09-20 18:28:28,155 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\nDevice set to use cpu\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport json # Import the json library\nfrom collections import Counter\nimport re\n\n# Define paths\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\nSTOCK_PATH = \"/kaggle/input/nse-stocks/EQUITY_L.csv\"\n\n# Input data paths\nchat_file = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/chat/Extracted messages....txt\"\nimage_folder = \"/kaggle/input/sebi-sample-input/dataset/stock_market_Trader_Public_Group/images\"\n\n# Entity information\nsebi_id_input = \"INZ000048660\"\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\ndef print_mobile_report(report):\n    \"\"\"\n    Formats the pipeline results into a clean, concise report suitable for a mobile app display.\n    \"\"\"\n    print(\"🚨 Fraud Detection Report\")\n\n    # --- Section 1: High-Priority Alerts ---\n    # This section shows the most critical information first.\n    user_report = report.get(\"user_prediction_accuracy_report\", {})\n    flagged_users = [user for user, data in user_report.items() if data.get(\"is_potential_scammer\")]\n    insider_flags = [f for f in report.get(\"insider_trading_flags\", []) if \"No insider\" not in f]\n    sentiment_flags = [f for f in report.get(\"sentiment_price_flags\", []) if \"No suspicious\" not in f]\n\n    if not flagged_users and not insider_flags and not sentiment_flags:\n        print(\"\\n✅ No high-priority alerts.\")\n    else:\n        for user in flagged_users:\n            stats = user_report[user]\n            print(f\"\\n⚠️ FLAGGED USER: '{user}' (Accuracy: {stats['accuracy_percent']}%)\")\n        for flag in insider_flags:\n            print(f\"\\n{flag}\") # The flag already includes an emoji\n        for flag in sentiment_flags:\n            print(f\"\\n{flag}\") # The flag already includes an emoji\n    \n    # --- Section 2: Relevant Messages ---\n    messages = report.get(\"message_by_message_analysis\", [])\n    if messages:\n        print(\"\\n\\n💬 Relevant Messages\")\n        print(\"---------------------\")\n        for msg in messages:\n            analysis = msg.get(\"analysis\", {})\n            print(f\"👤 {msg['user']} | 🗓️  {msg['date']}\")\n            print(f\"> \\\"{msg['message']}\\\"\")\n            \n            print(f\"  Scam Prediction: {analysis.get('text_scam_prediction', 'N/A')}\")\n\n            if \"url_analysis\" in analysis:\n                for url_res in analysis[\"url_analysis\"]:\n                    print(f\"  URL: {url_res['url']} (Risk: {url_res['risk_level']})\")\n            \n            if \"stock_mention_analysis\" in analysis:\n                stocks = ', '.join(analysis[\"stock_mention_analysis\"].get(\"mentioned_stocks\", []))\n                print(f\"  Stocks: {stocks}\")\n\n                verifications = analysis.get(\"stock_prediction_verification\", [])\n                for ver in verifications:\n                    pred_text = \"UP\" if re.search(r'\\b(up|increase|gain|rally|soar|\\+)\\b', ver['predicted_text'], re.IGNORECASE) else \"DOWN\"\n                    result_icon = \"✅\" if ver['prediction_correct'] else \"❌\"\n                    print(f\"  Verification ({ver['stock']}): Predicted {pred_text}, Actual {ver['actual_direction']} {result_icon}\")\n            print(\"---\")\n\n\n    # --- Section 3: Image Analysis ---\n    images = report.get(\"image_analysis\", [])\n    if images:\n        print(\"\\n🖼️ Image Analysis\")\n        print(\"------------------\")\n        for img in images:\n            img_analysis = img.get('analysis', {})\n            print(f\"File: {img['file_name']}\")\n            print(f\"  Text Scam: {img_analysis.get('text_scam_prediction', 'N/A')}\")\n            \n            qr = img_analysis.get(\"qr_analysis\")\n            if qr and qr.get('qr_data'):\n                qr_risk = qr.get('analysis', {})\n                print(f\"  QR Code: {qr['qr_data'][:50]}... (Risk: {qr_risk.get('risk_level', 'N/A')})\")\n            print(\"---\")\n\n    # --- Section 4: General Info ---\n    print(\"\\n📋 More Info\")\n    print(\"-------------\")\n    sebi_report = report.get(\"manual_verifications\", {}).get(\"sebi_id_analysis\", {})\n    print(f\"SEBI ID ({sebi_report.get('sebi_id', 'N/A')}): {sebi_report.get('status', 'N/A')}\")\n    bulk_deals_count = report.get(\"bulk_deal_analysis\", {}).get(\"trades_found_today\", 0)\n    print(f\"BSE Bulk Deals Today: {bulk_deals_count}\")\n# The print_pipeline_report function is no longer needed and has been removed.\n\n# Main execution\ntry:\n    # Assuming FraudDetectionPipeline class is defined in another cell or file\n    fraud_detector = FraudDetectionPipeline(\n        model_path=MODEL_PATH, \n        vectorizer_path=VECTORIZER_PATH, \n        stock_csv_path=STOCK_PATH\n    )\n    \n    if os.path.exists(chat_file):\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=sebi_id_input,\n        )\n        \n        # --- MODIFIED PART ---\n        # Instead of the custom print function, we now print the full JSON object.\n        # The `indent=4` argument makes the output readable.\n        print(\"\\n\" + \"=\"*70)\n        print(\"🚨 FULL PIPELINE REPORT (JSON)\")\n        print(\"=\"*70 + \"\\n\")\n        print(json.dumps(results, indent=4))\n        print(\"\\n\" + \"=\"*70)\n        print(\"✅ End of Report\")\n        print(\"=\"*70 + \"\\n\")\n\n    else:\n        print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")\n\nexcept NameError:\n    print(\"❌ ERROR: `FraudDetectionPipeline` class is not defined. Please ensure its code has been executed.\")\nexcept IOError as e:\n    print(e)\n    print(\"Please make sure the model and vectorizer files exist. You may need to train them first.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flask python-whois pyzbar pandas scikit-learn joblib opencv-python easyocr requests beautifulsoup4 html5lib yfinance -q\n\nimport os\nimport re\nimport json\nimport zipfile\nimport tempfile\nimport warnings\nfrom flask import Flask, request, jsonify\n\n# --- Suppress Warnings ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nwarnings.filterwarnings(\"ignore\")\n\n# --- Core Data Science & ML Imports ---\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# --- Image & Web Scraping Imports ---\nimport cv2\nimport easyocr\nimport whois\nimport requests\nimport yfinance as yf\n\nprint(\"✅ All libraries loaded successfully.\")\n\n# ===============================\n#   TRAINING FUNCTION\n# ===============================\ndef train_scam_classifier():\n    \"\"\"\n    Loads the dataset, trains a text classifier, and saves the model\n    and vectorizer to the /kaggle/working/ directory.\n    \"\"\"\n    print(\"🚀 Starting model training process...\")\n    try:\n        WORKING_DIR = \"/kaggle/working/\"\n        VECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\n        MODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n\n        df = pd.read_csv(\"/kaggle/input/whatsapp-scam/whatsapp_scam_dataset.csv\")\n\n        X = df[\"message\"]\n        y = df[\"scam_type\"]\n\n        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n        vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000, ngram_range=(1,2))\n        X_train_vec = vectorizer.fit_transform(X_train)\n\n        model = LogisticRegression(max_iter=200, class_weight=\"balanced\")\n        model.fit(X_train_vec, y_train)\n\n        joblib.dump(vectorizer, VECTORIZER_PATH)\n        joblib.dump(model, MODEL_PATH)\n        \n        print(f\"✅ Model and vectorizer saved successfully to {WORKING_DIR}\")\n        return True\n\n    except Exception as e:\n        print(f\"❌ Training failed: {e}\")\n        return False\n\n# Train once at startup\ntrain_scam_classifier()\n\n# ===============================\n#   SIMPLIFIER FUNCTION\n# ===============================\ndef simplify_report(report):\n    \"\"\"\n    Returns a simplified version of the pipeline report for presentation/demo.\n    \"\"\"\n    simplified = {\n        \"sebi_id_status\": report.get(\"manual_verifications\", {})\n                                 .get(\"sebi_id_analysis\", {})\n                                 .get(\"status\", \"N/A\"),\n        \"bulk_deals_today\": report.get(\"bulk_deal_analysis\", {})\n                                  .get(\"trades_found_today\", 0),\n        \"flagged_users\": {},\n        \"messages\": [],\n        \"images\": []\n    }\n\n    user_report = report.get(\"user_prediction_accuracy_report\", {})\n    for user, data in user_report.items():\n        if data.get(\"is_potential_scammer\"):\n            simplified[\"flagged_users\"][user] = {\n                \"accuracy_percent\": data.get(\"accuracy_percent\"),\n                \"correct_predictions\": data.get(\"correct_predictions\"),\n                \"incorrect_predictions\": data.get(\"incorrect_predictions\")\n            }\n\n    for msg in report.get(\"message_by_message_analysis\", []):\n        analysis = msg.get(\"analysis\", {})\n        if \"stock_mention_analysis\" in analysis or \"url_analysis\" in analysis:\n            simplified[\"messages\"].append({\n                \"date\": msg[\"date\"],\n                \"user\": msg[\"user\"],\n                \"message\": msg[\"message\"][:80] + (\"...\" if len(msg[\"message\"]) > 80 else \"\"),\n                \"scam_prediction\": analysis.get(\"text_scam_prediction\", \"N/A\"),\n                \"stocks\": analysis.get(\"stock_mention_analysis\", {})\n                                   .get(\"mentioned_stocks\", []),\n                \"urls\": [\n                    {\n                        \"url\": u[\"url\"],\n                        \"risk\": u[\"risk_level\"],\n                        \"score\": u[\"risk_score\"]\n                    }\n                    for u in analysis.get(\"url_analysis\", [])\n                ]\n            })\n\n    for img in report.get(\"image_analysis\", []):\n        simplified[\"images\"].append({\n            \"file_name\": img[\"file_name\"],\n            \"text_scam_prediction\": img.get(\"analysis\", {}).get(\"text_scam_prediction\", \"N/A\")\n        })\n\n    return simplified\n\n\"\"\"\napp.py\n\nFastAPI wrapper around the FraudDetectionPipeline.\n\nUsage:\n    pip install fastapi uvicorn python-whois pyzbar pandas scikit-learn joblib opencv-python easyocr requests beautifulsoup4 html5lib yfinance python-multipart -q\n    uvicorn app:app --host 0.0.0.0 --port 8000\n\"\"\"\n\nimport os\nimport re\nimport json\nimport glob\nimport shutil\nimport joblib\nimport whois\nimport cv2\nimport easyocr\nimport requests\nimport datetime\nimport pandas as pd\nfrom io import StringIO\nfrom urllib.parse import urlparse\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom transformers import pipeline\nimport yfinance as yf\n\nfrom fastapi import FastAPI, UploadFile, File, Form, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom typing import List, Optional\nimport tempfile\n\n# ----------------------------\n# FraudDetectionPipeline class\n# (Full class, adapted from the pipeline you used)\n# ----------------------------\nclass FraudDetectionPipeline:\n    \"\"\"\n    Fully integrated multi-modal fraud detection pipeline:\n    - Chat analysis\n    - Stock mention analysis\n    - Stock prediction verification (uses yfinance)\n    - URL and QR risk detection\n    - Image OCR analysis (easyocr)\n    - Insider trading alerts (CSV)\n    - Sentiment vs price check (transformers + yfinance)\n    \"\"\"\n\n    def __init__(self, model_path, vectorizer_path, stock_csv_path=None):\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        # load model + vectorizer\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n        except Exception as e:\n            raise IOError(f\"Failed to load model/vectorizer: {e}\")\n\n        # OCR reader\n        try:\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False)\n        except Exception:\n            # If easyocr fails to init (missing fonts etc.), keep None and skip OCR\n            self.ocr_reader = None\n\n        # load ticker list if provided\n        self.ticker_set = set()\n        self.company_name_map = {}\n        if stock_csv_path and os.path.exists(stock_csv_path):\n            try:\n                stock_df = pd.read_csv(stock_csv_path, dtype=str, low_memory=False)\n                # Normalize column names\n                cols = {c.strip().upper(): c for c in stock_df.columns}\n                # Try common column names\n                symbol_col = cols.get(\"SYMBOL\", None)\n                name_col = cols.get(\"NAME OF COMPANY\", None) or cols.get(\"NAME\", None)\n                if symbol_col:\n                    self.ticker_set = set(stock_df[symbol_col].str.upper().dropna().astype(str).tolist())\n                if name_col and symbol_col:\n                    self.company_name_map = dict(zip(stock_df[name_col].str.lower().dropna().astype(str),\n                                                     stock_df[symbol_col].str.upper().astype(str)))\n            except Exception:\n                self.ticker_set = set()\n                self.company_name_map = {}\n\n    # URL risk\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"):\n                score += 30\n                reasons.append(\"Not HTTPS\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain):\n                score += 40\n                reasons.append(\"IP address used\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]):\n                score += 25\n                reasons.append(\"Suspicious TLD\")\n            try:\n                domain_info = whois.whois(domain)\n                creation_date = domain_info.creation_date\n                if creation_date:\n                    creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                    age_days = (datetime.datetime.now() - creation_date).days\n                    if age_days < 180:\n                        score += 30\n                        reasons.append(f\"Domain created {age_days} days ago\")\n            except Exception:\n                score += 10\n                reasons.append(\"WHOIS failed\")\n        except Exception:\n            return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"Malformed URL\"]}\n        risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n    # QR analysis\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None:\n                return None\n            qr_detector = cv2.QRCodeDetector()\n            data, _, _ = qr_detector.detectAndDecode(img)\n            if data:\n                return {\n                    \"qr_data\": data,\n                    \"analysis\": self._check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR non-URL text\"]}\n                }\n        except Exception:\n            return None\n        return None\n\n    # OCR image analysis\n    def _analyze_image_content(self, image_path):\n        results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": self._check_qr_code(image_path)}\n        try:\n            if self.ocr_reader:\n                extracted_text = \" \".join(self.ocr_reader.readtext(image_path, detail=0, paragraph=True))\n                if extracted_text.strip():\n                    results[\"ocr_text\"] = extracted_text\n                    vec = self.vectorizer.transform([extracted_text])\n                    results[\"text_scam_prediction\"] = self.model.predict(vec)[0]\n        except Exception:\n            pass\n        return results\n\n    # SEBI id (dummy)\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id:\n            return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n        return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\n    # stock detector\n    def detect_stocks_in_text(self, text):\n        found_stocks = set()\n        text_lower = text.lower()\n        for name, symbol in self.company_name_map.items():\n            if name in text_lower:\n                found_stocks.add(symbol)\n        words = re.findall(r'\\b\\w+\\b', text)\n        for word in words:\n            if word.upper() in self.ticker_set:\n                found_stocks.add(word.upper())\n        return list(found_stocks)\n\n    def _analyze_stock_mentions(self, text):\n        detected_stocks = self.detect_stocks_in_text(text)\n        if not detected_stocks:\n            return None\n        suspicious_patterns = []\n        if any(kw in text.lower() for kw in [\"guaranteed\", \"10x\", \"insider\", \"lottery\", \"free\"]):\n            suspicious_patterns.append(\"pump-and-dump/marketing keywords present\")\n        return {\"mentioned_stocks\": detected_stocks, \"suspicious_patterns\": suspicious_patterns}\n\n    # verify prediction for a message (single)\n    def _verify_stock_prediction(self, message_text, mentioned_stocks, message_date, horizon_days=3):\n        verification_results = []\n        try:\n            ts = pd.to_datetime(message_date)\n            tickers = [s + \".NS\" for s in mentioned_stocks]\n            # yfinance download range: include start and horizon end\n            start = ts.date()\n            end = (ts + pd.Timedelta(days=horizon_days)).date()\n            raw = yf.download(tickers, start=start, end=end, progress=False, group_by='ticker')\n            # Normalize retrieved data: raw can be multi-index or single df\n            for stock in mentioned_stocks:\n                tkr = stock + \".NS\"\n                df = None\n                if isinstance(raw, dict):\n                    df = raw.get(tkr)\n                else:\n                    # sometimes yfinance returns single ticker df\n                    if tkr in raw.columns.get_level_values(0) if hasattr(raw.columns, \"levels\") else False:\n                        df = raw[tkr]\n                    elif \"Close\" in raw.columns and len(tickers) == 1:\n                        df = raw\n                if df is None or df.empty:\n                    continue\n                # need at least two rows to compare\n                if len(df[\"Close\"]) < 2:\n                    continue\n                change = df[\"Close\"].iloc[-1] - df[\"Close\"].iloc[0]\n                actual_direction = \"UP\" if change > 0 else \"DOWN\"\n                predicted_up = bool(re.search(r'\\b(up|increase|gain|rally|soar|\\+|target)\\b', message_text, re.IGNORECASE))\n                predicted_down = bool(re.search(r'\\b(down|fall|drop|loss|-|sell)\\b', message_text, re.IGNORECASE))\n                match = (predicted_up and change > 0) or (predicted_down and change < 0)\n                verification_results.append({\n                    \"stock\": stock,\n                    \"predicted_text\": message_text,\n                    \"actual_change\": float(change),\n                    \"actual_direction\": actual_direction,\n                    \"prediction_correct\": bool(match)\n                })\n        except Exception:\n            pass\n        return verification_results if verification_results else None\n\n    # parse chat file or chat text\n    def _parse_chat_for_messages(self, chat_file_path=None, chat_text=None, user_filter=\"\"):\n        messages = []\n        # pattern supports timestamps like '2025-09-07 12:34:56+05:30 - user: message'\n        pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(?:[+]\\d{2}:\\d{2})?)\\s*-\\s*(.*?):\\s*(.*)')\n        if chat_text:\n            lines = chat_text.splitlines()\n        elif chat_file_path and os.path.exists(chat_file_path):\n            with open(chat_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                lines = f.readlines()\n        else:\n            return []\n        for line in lines:\n            match = pattern.match(line.strip())\n            if match:\n                timestamp, user, message = match.groups()\n                if user_filter == \"\" or user_filter.lower() in user.lower():\n                    messages.append({\"date\": timestamp, \"user\": user, \"message\": message})\n        return messages\n\n    # bot detection\n    def _detect_bot_behavior(self, file_path_or_text):\n        # simple counts by user\n        if os.path.exists(file_path_or_text):\n            with open(file_path_or_text, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                lines = f.readlines()\n        else:\n            lines = file_path_or_text.splitlines() if isinstance(file_path_or_text, str) else []\n        users = [re.match(r\".*?-\\s(.*?):\", line).group(1).strip()\n                 for line in lines if re.match(r\".*?-\\s(.*?):\", line)]\n        if not users:\n            return {\"error\": \"No users found.\"}\n        counts = Counter(users)\n        return {\"detected_admin\": counts.most_common(1)[0][0], \"message_counts\": dict(counts)}\n\n    # simplified sentiment-price check (kept lightweight)\n    def _sentiment_price_check(self, scam_messages):\n        try:\n            sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n        except Exception:\n            sentiment_analyzer = None\n        alerts = []\n        all_stocks = set()\n        for msg in scam_messages:\n            s = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if s:\n                all_stocks.update(s.get(\"mentioned_stocks\", []))\n        if not all_stocks or sentiment_analyzer is None:\n            return [\"No suspicious sentiment/price mismatches.\"]\n        tickers = [s + \".NS\" for s in all_stocks]\n        try:\n            price_data = yf.download(tickers, period=\"5d\", interval=\"1d\", group_by='ticker', progress=False)\n        except Exception:\n            price_data = pd.DataFrame()\n        for msg in scam_messages:\n            s = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not s:\n                continue\n            sentiment = sentiment_analyzer(msg[\"message\"][:250])[0]\n            label = sentiment[\"label\"].lower()\n            for stock in s[\"mentioned_stocks\"]:\n                try:\n                    key = stock + \".NS\"\n                    if key in price_data:\n                        df = price_data[key]\n                        if not df.empty and len(df[\"Close\"]) > 1:\n                            change = df[\"Close\"].iloc[-1] - df[\"Close\"].iloc[-2]\n                            if \"positive\" in label and change < 0:\n                                alerts.append(f\"Positive sentiment for {stock} but price fell.\")\n                            if \"negative\" in label and change > 0:\n                                alerts.append(f\"Negative sentiment for {stock} but price rose.\")\n                except Exception:\n                    continue\n        return alerts if alerts else [\"No suspicious sentiment/price mismatches.\"]\n\n    # get bse bulk deals (best-effort)\n    def _get_bse_bulk_deals(self):\n        try:\n            url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            for table in pd.read_html(StringIO(response.text)):\n                if \"Client Name\" in table.columns:\n                    table.columns = table.columns.str.strip()\n                    table[\"Deal Date\"] = pd.to_datetime(table[\"Deal Date\"], format=\"%d/%m/%Y\", errors='coerce')\n                    return table\n        except Exception:\n            return pd.DataFrame()\n        return pd.DataFrame()\n\n    # insider trading check (expects DataFrame with expected column names)\n    def _check_insider_trading(self, scam_messages, insider_df, lookback_days=7):\n        alerts = []\n        if insider_df is None or insider_df.empty:\n            return [\"No insider data provided.\"]\n        for msg in scam_messages:\n            s = msg.get(\"analysis\", {}).get(\"stock_mention_analysis\")\n            if not s:\n                continue\n            for stock in s[\"mentioned_stocks\"]:\n                try:\n                    df_filtered = insider_df[\n                        insider_df['SYMBOL'].astype(str).str.upper() == stock.upper()\n                    ]\n                    # filter by recent date if column exists (best-effort)\n                    if 'DATE OF ALLOTMENT/ACQUISITION FROM' in insider_df.columns:\n                        df_filtered = df_filtered[\n                            pd.to_datetime(df_filtered['DATE OF ALLOTMENT/ACQUISITION FROM'], errors='coerce')\n                            >= (pd.Timestamp.now() - pd.Timedelta(days=lookback_days))\n                        ]\n                    for _, tr in df_filtered.iterrows():\n                        alerts.append(f\"Insider {tr.get('NAME OF THE ACQUIRER/DISPOSER','?')} traded {tr.get('SYMBOL','?')}\")\n                except Exception:\n                    continue\n        return alerts if alerts else [\"No insider red flags detected.\"]\n\n    # run full pipeline (entry point)\n    def run(self, chat_file_path=None, chat_text=None, image_folder_path=None,\n            sebi_id_to_check=None, insider_csv_path=None, user_filter=\"\"):\n        final_report = defaultdict(dict)\n        final_report[\"manual_verifications\"] = {\"sebi_id_analysis\": self._verify_sebi_id(sebi_id_to_check)}\n        bulk_deals_df = self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": int(len(bulk_deals_df))}\n\n        # parse messages\n        messages = self._parse_chat_for_messages(chat_file_path=chat_file_path, chat_text=chat_text, user_filter=user_filter)\n        final_report[\"message_by_message_analysis\"] = []\n\n        # prepare messages with analysis\n        for m in messages:\n            text = m[\"message\"]\n            scam_pred = \"N/A\"\n            try:\n                vec = self.vectorizer.transform([text])\n                scam_pred = self.model.predict(vec)[0]\n            except Exception:\n                scam_pred = \"N/A\"\n            analysis = {\"text_scam_prediction\": scam_pred}\n\n            # url analysis\n            urls = re.findall(r\"(https?://\\S+)\", text)\n            if urls:\n                analysis[\"url_analysis\"] = [self._check_url_risk(u) for u in urls]\n\n            # stock mentions\n            stock_analysis = self._analyze_stock_mentions(text)\n            if stock_analysis:\n                analysis[\"stock_mention_analysis\"] = stock_analysis\n                # prediction verification\n                ver = self._verify_stock_prediction(text, stock_analysis[\"mentioned_stocks\"], m[\"date\"])\n                if ver:\n                    analysis[\"stock_prediction_verification\"] = ver\n\n            m[\"analysis\"] = analysis\n            final_report[\"message_by_message_analysis\"].append(m)\n\n        # insider CSV\n        if insider_csv_path and os.path.exists(insider_csv_path):\n            try:\n                insider_df = pd.read_csv(insider_csv_path, dtype=str, low_memory=False)\n            except Exception:\n                insider_df = pd.DataFrame()\n        else:\n            insider_df = pd.DataFrame()\n\n        final_report[\"insider_trading_flags\"] = self._check_insider_trading(final_report[\"message_by_message_analysis\"], insider_df)\n        final_report[\"sentiment_price_flags\"] = self._sentiment_price_check(final_report[\"message_by_message_analysis\"])\n\n        # images (scan folder if provided)\n        final_report[\"image_analysis\"] = []\n        if image_folder_path and os.path.exists(image_folder_path):\n            image_files = glob.glob(os.path.join(image_folder_path, \"*\"))\n            for img in image_files:\n                img_report = self._analyze_image_content(img)\n                final_report[\"image_analysis\"].append({\"file_name\": os.path.basename(img), \"analysis\": img_report})\n\n        return final_report\n\n# ----------------------------\n# simplify_report helper (to return a small JSON)\n# ----------------------------\ndef simplify_report(report):\n    simplified = {\n        \"sebi_id_status\": report.get(\"manual_verifications\", {}).get(\"sebi_id_analysis\", {}).get(\"status\", \"N/A\"),\n        \"bulk_deals_today\": report.get(\"bulk_deal_analysis\", {}).get(\"trades_found_today\", 0),\n        \"messages\": [],\n        \"images\": []\n    }\n    for msg in report.get(\"message_by_message_analysis\", []):\n        analysis = msg.get(\"analysis\", {})\n        if \"stock_mention_analysis\" in analysis or \"url_analysis\" in analysis:\n            simplified[\"messages\"].append({\n                \"date\": msg.get(\"date\"),\n                \"user\": msg.get(\"user\"),\n                \"message\": (msg.get(\"message\") or \"\")[:120] + (\"...\" if len(msg.get(\"message\",\"\")) > 120 else \"\"),\n                \"scam_prediction\": analysis.get(\"text_scam_prediction\", \"N/A\"),\n                \"stocks\": analysis.get(\"stock_mention_analysis\", {}).get(\"mentioned_stocks\", []),\n                \"urls\": [{\"url\": u[\"url\"], \"risk\": u[\"risk_level\"], \"score\": u[\"risk_score\"]} for u in analysis.get(\"url_analysis\",[])]\n            })\n    for img in report.get(\"image_analysis\", []):\n        simplified[\"images\"].append({\"file_name\": img.get(\"file_name\"), \"text_scam_prediction\": img.get(\"analysis\",{}).get(\"text_scam_prediction\",\"N/A\")})\n    return simplified\n\n# ----------------------------\n# FastAPI app\n# ----------------------------\napp = FastAPI(title=\"Fraud Detection Pipeline API\")\n\n# Allow all origins by default (change to specific domains in production)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Paths to your saved model files (update if different)\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n# STOCK_PATH can be omitted if not available\nSTOCK_PATH = \"/kaggle/input/nse-stocks/EQUITY_L.csv\"\n\n# Load pipeline once at startup\nPIPELINE = None\n@app.on_event(\"startup\")\ndef load_pipeline():\n    global PIPELINE\n    if PIPELINE is None:\n        # Try to initialize; if it fails the endpoints will return error\n        try:\n            PIPELINE = FraudDetectionPipeline(model_path=MODEL_PATH, vectorizer_path=VECTORIZER_PATH, stock_csv_path=STOCK_PATH)\n        except Exception as e:\n            print(f\"Failed to initialize pipeline on startup: {e}\")\n            PIPELINE = None\n\n@app.get(\"/status\")\ndef status():\n    ok = PIPELINE is not None\n    return {\"status\": \"ok\" if ok else \"error\", \"pipeline_loaded\": ok}\n\n@app.post(\"/analyze\")\ndef analyze(\n    chat_text: Optional[str] = Form(None),\n    chat_file_path: Optional[str] = Form(None),\n    image_folder_path: Optional[str] = Form(None),\n    sebi_id: Optional[str] = Form(None),\n    insider_csv_path: Optional[str] = Form(None),\n    user_filter: Optional[str] = Form(\"\")\n):\n    \"\"\"\n    Analyze chat (either provide chat_text OR chat_file_path)\n    Optionally provide image_folder_path (server-accessible path) or insider_csv_path.\n    Returns full report and a simplified report.\n    \"\"\"\n    if PIPELINE is None:\n        raise HTTPException(status_code=500, detail=\"Pipeline not loaded on server.\")\n\n    if not chat_text and not chat_file_path:\n        raise HTTPException(status_code=400, detail=\"Provide chat_text or chat_file_path\")\n\n    # Run pipeline\n    try:\n        report = PIPELINE.run(chat_file_path=chat_file_path, chat_text=chat_text, image_folder_path=image_folder_path,\n                              sebi_id_to_check=sebi_id, insider_csv_path=insider_csv_path, user_filter=user_filter)\n        simplified = simplify_report(report)\n        return {\"status\": \"ok\", \"report\": report, \"simplified\": simplified}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Analysis failed: {e}\")\n\n@app.post(\"/analyze_uploads\")\nasync def analyze_uploads(\n    chat_file: Optional[UploadFile] = File(None),\n    images: Optional[List[UploadFile]] = File(None),\n    sebi_id: Optional[str] = Form(None),\n    insider_csv: Optional[UploadFile] = File(None),\n    user_filter: Optional[str] = Form(\"\")\n):\n    \"\"\"\n    Accept local uploads (chat file, multiple images, optional insider CSV).\n    Saves uploads to a temp dir, runs the pipeline, returns results.\n    \"\"\"\n    if PIPELINE is None:\n        raise HTTPException(status_code=500, detail=\"Pipeline not loaded on server.\")\n\n    tmpdir = tempfile.mkdtemp(prefix=\"fraud_api_\")\n    try:\n        chat_file_path = None\n        if chat_file:\n            chat_file_path = os.path.join(tmpdir, chat_file.filename)\n            with open(chat_file_path, \"wb\") as f:\n                f.write(await chat_file.read())\n\n        image_folder = None\n        if images:\n            image_folder = os.path.join(tmpdir, \"images\")\n            os.makedirs(image_folder, exist_ok=True)\n            for up in images:\n                dst = os.path.join(image_folder, up.filename)\n                with open(dst, \"wb\") as f:\n                    f.write(await up.read())\n\n        insider_csv_path = None\n        if insider_csv:\n            insider_csv_path = os.path.join(tmpdir, insider_csv.filename)\n            with open(insider_csv_path, \"wb\") as f:\n                f.write(await insider_csv.read())\n\n        report = PIPELINE.run(chat_file_path=chat_file_path, chat_text=None,\n                              image_folder_path=image_folder,\n                              sebi_id_to_check=sebi_id,\n                              insider_csv_path=insider_csv_path,\n                              user_filter=user_filter)\n        simplified = simplify_report(report)\n        return {\"status\": \"ok\", \"report\": report, \"simplified\": simplified}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Upload analysis failed: {e}\")\n    finally:\n        # clean up temporary dir\n        try:\n            shutil.rmtree(tmpdir)\n        except Exception:\n            pass\n\n# If running this file directly, start uvicorn (optional)\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=False)\n\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\nSTOCK_PATH = \"/kaggle/input/nse-stocks/EQUITY_L.csv\"\n\nfraud_detector = FraudDetectionPipeline(\n    model_path=MODEL_PATH,\n    vectorizer_path=VECTORIZER_PATH,\n    stock_csv_path=STOCK_PATH\n)\n\napp = Flask(__name__)\n\n@app.route(\"/analyze\", methods=[\"POST\"])\ndef analyze_zip():\n    \"\"\"\n    Expects a ZIP file upload containing:\n    - One chat .txt file\n    - Multiple .png images\n    \"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file uploaded\"}), 400\n\n    file = request.files['file']\n    if not file.filename.endswith(\".zip\"):\n        return jsonify({\"error\": \"Only ZIP files are allowed\"}), 400\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        zip_path = os.path.join(tmpdir, \"input.zip\")\n        file.save(zip_path)\n\n        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(tmpdir)\n\n        chat_file = None\n        image_folder = os.path.join(tmpdir, \"images\")\n        os.makedirs(image_folder, exist_ok=True)\n\n        for root, _, files in os.walk(tmpdir):\n            for f in files:\n                if f.endswith(\".txt\"):\n                    chat_file = os.path.join(root, f)\n                elif f.endswith(\".png\"):\n                    os.rename(os.path.join(root, f), os.path.join(image_folder, f))\n\n        if not chat_file:\n            return jsonify({\"error\": \"No .txt chat file found in ZIP\"}), 400\n\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=\"INZ000048660\"  # static for now\n        )\n\n        return jsonify(simplify_report(results))\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}