{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13005109,"sourceType":"datasetVersion","datasetId":8233187},{"sourceId":13005413,"sourceType":"datasetVersion","datasetId":8233394},{"sourceId":13005512,"sourceType":"datasetVersion","datasetId":8233460},{"sourceId":13009858,"sourceType":"datasetVersion","datasetId":8236577},{"sourceId":13010090,"sourceType":"datasetVersion","datasetId":8236723}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-whois pyzbar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:19.681023Z","iopub.execute_input":"2025-09-10T04:31:19.681677Z","iopub.status.idle":"2025-09-10T04:31:25.055918Z","shell.execute_reply.started":"2025-09-10T04:31:19.681655Z","shell.execute_reply":"2025-09-10T04:31:25.055197Z"}},"outputs":[{"name":"stdout","text":"Collecting python-whois\n  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\nCollecting pyzbar\n  Downloading pyzbar-0.1.9-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from python-whois) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->python-whois) (1.17.0)\nDownloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzbar-0.1.9-py2.py3-none-any.whl (32 kB)\nInstalling collected packages: pyzbar, python-whois\nSuccessfully installed python-whois-0.9.5 pyzbar-0.1.9\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom io import StringIO\n\ndef get_bulk_deal_client_frequency():\n    \"\"\"\n    Fetches daily bulk deal data from the BSE and calculates the frequency\n    of each client name involved in the trades.\n    \"\"\"\n    print(\"Fetching daily bulk deal data from BSE...\")\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        all_tables = pd.read_html(StringIO(response.text))\n        \n        deals_df = None\n        for table in all_tables:\n            if 'Client Name' in table.columns:\n                deals_df = table\n                break\n        \n        if deals_df is None or deals_df.empty:\n            print(\"❌ Could not find or parse the bulk deals table today.\")\n            return None\n\n        # Use value_counts() to get the frequency of each name\n        name_counts = deals_df['Client Name'].value_counts()\n        print(\"✅ Successfully calculated client name frequencies.\")\n        return name_counts\n\n    except Exception as e:\n        print(f\"❌ An error occurred: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    client_frequencies = get_bulk_deal_client_frequency()\n    \n    if client_frequencies is not None and not client_frequencies.empty:\n        print(\"\\n\" + \"=\"*50)\n        print(\"Frequency of Client Names in Today's Bulk Deals\")\n        print(\"=\"*50)\n        # The result from value_counts() is a pandas Series\n        for name, count in client_frequencies.items():\n            print(f\"- {name}: {count} trade(s)\")\n    else:\n        print(\"\\nNo bulk deals to analyze today.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:25.057525Z","iopub.execute_input":"2025-09-10T04:31:25.057784Z","iopub.status.idle":"2025-09-10T04:31:28.352538Z","shell.execute_reply.started":"2025-09-10T04:31:25.057760Z","shell.execute_reply":"2025-09-10T04:31:28.351750Z"}},"outputs":[{"name":"stdout","text":"Fetching daily bulk deal data from BSE...\n✅ Successfully calculated client name frequencies.\n\n==================================================\nFrequency of Client Names in Today's Bulk Deals\n==================================================\n- IRAGE BROKING SERVICES LLP: 6 trade(s)\n- NEO APEX VENTURE LLP: 5 trade(s)\n- MANSI SHARE AND STOCK BROKING PRIVATE LIMITED: 4 trade(s)\n- NEO APEX SHARE BROKING SERVICES LLP: 4 trade(s)\n- PRAS INVESTMENT PRIVATE LIMITED: 3 trade(s)\n- PARNIT VENTURES PRIVATE LIMITED: 2 trade(s)\n- B N RATHI SECURITIES LIMITED: 2 trade(s)\n- AKSHAY PALIWAL: 2 trade(s)\n- SYLPH TECHNOLOGIES LIMITED: 2 trade(s)\n- SAROJDEVI P GUPTA: 2 trade(s)\n- BHATIA NIKHIL MURLIDHAR: 2 trade(s)\n- ISHAAN TRADEFIN LLP: 2 trade(s)\n- F3 ADVISORS PRIVATE LIMITED: 2 trade(s)\n- QE SECURITIES LLP: 2 trade(s)\n- VARANGA PROPERTIES PRIVATE LIMITED: 2 trade(s)\n- PRASHANT GUPTA: 2 trade(s)\n- VIKRAMKUMAR KARANRAJ SAKARIA HUF: 2 trade(s)\n- CHAUHAN NAGJIBHAI CHANDUBHAI: 2 trade(s)\n- KAMLESH NAVINCHANDRA SHAH: 2 trade(s)\n- SHAILESH DHAMELIYA: 2 trade(s)\n- SHARE INDIA SECURITIES LIMITED: 2 trade(s)\n- NEOMILE CORPORATE ADVISORY PRIVATE LIMITED: 2 trade(s)\n- ALACRITY SECURITIES LIMITED: 2 trade(s)\n- NIRAJ RAJNIKANT SHAH: 2 trade(s)\n- GURVINDER SINGH: 2 trade(s)\n- VAGHANI VIRAJ: 1 trade(s)\n- MMD SECURITIES PVT LTD: 1 trade(s)\n- ARCHANA WADHWA: 1 trade(s)\n- ANKITA VISHAL SHAH: 1 trade(s)\n- PREMILA ASHWIN DOSHI: 1 trade(s)\n- JAIN SATISH SAGARMAL: 1 trade(s)\n- MAHENDRAKUMAR S JAIN: 1 trade(s)\n- MAYURI SHRIPAL VORA: 1 trade(s)\n- VANDANA AGARWALA: 1 trade(s)\n- BHASKARA HARINATH PREETHAM: 1 trade(s)\n- FIRST AMONG EQUALS: 1 trade(s)\n- NARESHKUMAR MOHANLAL PATEL: 1 trade(s)\n- ZAINAL KHAN: 1 trade(s)\n- ANKITGERA: 1 trade(s)\n- BRIJESH D PATEL HUF: 1 trade(s)\n- JYOTI K SHAH: 1 trade(s)\n- FAIJAL GAFARBHAI KHATRI: 1 trade(s)\n- SAHAJ TRADING AGENCY: 1 trade(s)\n- NARENDRAKUMAR GANGARAMDAS PATEL: 1 trade(s)\n- DHRUMIL RAKESH PATEL: 1 trade(s)\n- RAKESHBHAI RAMESHBHAI PATEL: 1 trade(s)\n- JUPITER CITY DEVELOPERS (INDIA) LIMITED: 1 trade(s)\n- AKARSHIKA TRADERS LLP: 1 trade(s)\n- TECHNOCRAVES SOLUTIONS PRIVATE LIMITED: 1 trade(s)\n- COMFORT ADVERTISING PVT LTD: 1 trade(s)\n- L7 HITECH PRIVATE LIMITED: 1 trade(s)\n- SHYAMA SHAHI: 1 trade(s)\n- LLOYDS ENTERPRISES LIMITED: 1 trade(s)\n- SEEMA RAGHUNATH AGGARWAL: 1 trade(s)\n- BIPINBHAI DEVABHAI RAVAL: 1 trade(s)\n- SHAH SHARAD KANAYALAL: 1 trade(s)\n- KAMLESHKUMAR JAGDISHBHAI PATEL: 1 trade(s)\n- KISHANKAMLESHKUMARPATEL: 1 trade(s)\n- TULASI MEENA: 1 trade(s)\n- CHIRAG HASMUKH SACHDEV: 1 trade(s)\n- PARMAR CHANDUBHAI: 1 trade(s)\n- DEALMONEY COMMODITIES PRIVATE LIMITED: 1 trade(s)\n- JIGNA MUKESH SHAH: 1 trade(s)\n- JYOTI KHANDELWAL: 1 trade(s)\n- APPLE EQUIFIN PVT LTD: 1 trade(s)\n- NIMIT JAYENDRA SHAH: 1 trade(s)\n- NATVARSINH TAKHUJI CHAVDA: 1 trade(s)\n- DARSHI ATULKUMAR SHAH: 1 trade(s)\n- GIRABEN ATULBHAI SHAH: 1 trade(s)\n- VENKATESH PRASHANTH: 1 trade(s)\n- GOVINDSINGH BHAVNATHSINGH RAJBHAR: 1 trade(s)\n- MANISH RAJPUT: 1 trade(s)\n- MOHAN SINGH SENGAR: 1 trade(s)\n- PARIJATA TRADING PRIVATE LIMITED: 1 trade(s)\n- AVANEESH TRADING PRIVATE LIMITED: 1 trade(s)\n- MONA LAROIA: 1 trade(s)\n- YUGA STOCKS AND COMMODITIES PRIVATE LIMITED: 1 trade(s)\n- RADADIYA CHINTANBHAI ANILBHAI: 1 trade(s)\n- PATEL SHIVLAL KUBERBHAI: 1 trade(s)\n- SAURABH JAIN: 1 trade(s)\n- MANDAKINIBEN PRADYUMANBHAI PATEL: 1 trade(s)\n- UNIQUE SOLUTIONS: 1 trade(s)\n- RAVINDRA HASTIMAL SAKARIYA: 1 trade(s)\n- VEER RAVINDRA SAKARIYA: 1 trade(s)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import requests\nimport pandas as pd\nimport re\nfrom io import StringIO\nfrom datetime import datetime\n\ndef get_bse_bulk_deals():\n    \"\"\"Fetches and parses daily bulk deal data from the BSE website.\"\"\"\n    print(\"Fetching daily bulk deal data...\")\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {'User-Agent': 'Mozilla/5.0'}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        all_tables = pd.read_html(StringIO(response.text))\n        for table in all_tables:\n            if 'Client Name' in table.columns and 'Deal Date' in table.columns:\n                print(\"✅ Bulk deal data fetched.\")\n                table.columns = table.columns.str.strip()\n                # Convert date columns to datetime objects for comparison\n                table['Deal Date'] = pd.to_datetime(table['Deal Date'], format='%d/%m/%Y')\n                return table\n        return pd.DataFrame()\n    except Exception as e:\n        print(f\"❌ Error fetching BSE bulk deal data: {e}\")\n        return pd.DataFrame()\n\ndef parse_chat_for_advisor_mentions(file_path, advisor_name):\n    \"\"\"\n    Parses a chat log to find all stock mentions made by a specific advisor.\n    A stock is assumed to be any word in all caps with 3 or more letters.\n    \"\"\"\n    print(f\"Parsing chat log for messages by '{advisor_name}'...\")\n    stock_mentions = []\n    stock_pattern = re.compile(r'\\b[A-Z]{3,}\\b')\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            # Match lines that contain a date, time, and the specific advisor's name\n            match = re.match(r'(\\d{2}/\\d{2}/\\d{2,4}),\\s\\d{2}:\\d{2}\\s-\\s(.*?):\\s(.*)', line)\n            if match and advisor_name.lower() in match.group(2).lower():\n                date_str, user, message = match.groups()\n                message_date = datetime.strptime(date_str, '%d/%m/%y') # Assuming dd/mm/yy format\n                \n                # Find all potential stock symbols in the message\n                symbols = stock_pattern.findall(message)\n                for symbol in symbols:\n                    stock_mentions.append({\n                        \"date\": message_date,\n                        \"stock_symbol\": symbol,\n                        \"message\": message.strip()\n                    })\n    print(f\"✅ Found {len(stock_mentions)} stock mentions by the advisor.\")\n    return stock_mentions\n\ndef cross_correlation_engine(chat_file, advisor_name):\n    \"\"\"\n    The main engine to correlate chat mentions with bulk trades.\n    \"\"\"\n    # Step 1: Get all recent bulk trades\n    trades_df = get_bse_bulk_deals()\n    if trades_df.empty:\n        return [\"Could not retrieve bulk deal data to perform analysis.\"]\n\n    # Filter for trades made only by our target advisor\n    advisor_trades = trades_df[trades_df['Client Name'].str.contains(advisor_name, case=False, na=False)].copy()\n    if advisor_trades.empty:\n        return [f\"No recent bulk deals found for '{advisor_name}'. No correlation possible.\"]\n    \n    # Step 2: Get all stock mentions by the advisor from the chat log\n    advisor_mentions = parse_chat_for_advisor_mentions(chat_file, advisor_name)\n    if not advisor_mentions:\n        return [f\"No stock mentions found for '{advisor_name}' in the chat log.\"]\n        \n    # Step 3: Correlate mentions and trades to find red flags\n    print(\"\\nCorrelating trades and chat messages...\")\n    red_flags = []\n    \n    for mention in advisor_mentions:\n        for _, trade in advisor_trades.iterrows():\n            # Check if the mentioned stock name is part of the security name in the trade data\n            if mention['stock_symbol'].lower() in trade['Security Name'].lower():\n                time_delta = mention['date'] - trade['Deal Date']\n                \n                # --- Red Flag 1: Front-Running ---\n                # Promotion happens within 7 days AFTER a bulk BUY\n                if trade['Deal Type'].lower() == 'buy' and 0 <= time_delta.days <= 7:\n                    flag = (f\"🚨 POTENTIAL FRONT-RUNNING DETECTED:\\n\"\n                            f\"  -> Advisor promoted '{mention['stock_symbol']}' on {mention['date'].date()}\\n\"\n                            f\"  -> This was just {time_delta.days} day(s) AFTER their bulk BUY of {trade['Quantity']} shares on {trade['Deal Date'].date()}.\\n\")\n                    red_flags.append(flag)\n\n                # --- Red Flag 2: Pump & Dump ---\n                # A bulk SELL happens within 30 days AFTER a promotion\n                if trade['Deal Type'].lower() == 'sell' and 0 <= time_delta.days <= 30:\n                    flag = (f\"🚨 POTENTIAL PUMP & DUMP DETECTED:\\n\"\n                            f\"  -> Advisor made a bulk SELL of {trade['Quantity']} shares of {trade['Security Name']} on {trade['Deal Date'].date()}\\n\"\n                            f\"  -> This was {time_delta.days} day(s) AFTER they promoted '{mention['stock_symbol']}' on {mention['date'].date()}.\\n\")\n                    red_flags.append(flag)\n\n    return red_flags if red_flags else [\"No suspicious correlations found between chat messages and recent bulk deals.\"]\n\nif __name__ == \"__main__\":\n    # --- 🔽 INPUTS FOR THE ANALYSIS 🔽 ---\n    # 1. Path to your chat log file\n    chat_log_file = \"/kaggle/input/whatsap-chats-downloaded/WhatsApp Chat with TriDevs.txt\" #<-- CHANGE THIS\n    \n    # 2. The exact name of the advisor as it appears in the chat and bulk deal data\n    advisor_name_to_investigate = \"NEO APEX VENTURE LLP\" #<-- CHANGE THIS\n    \n    # --- Run the engine ---\n    if not os.path.exists(chat_log_file):\n        print(f\"❌ ERROR: Chat file not found at '{chat_log_file}'\")\n    else:\n        suspicious_activities = cross_correlation_engine(chat_log_file, advisor_name_to_investigate)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"          Cross-Correlation Analysis Report\")\n        print(\"=\"*80)\n        for activity in suspicious_activities:\n            print(activity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:28.353543Z","iopub.execute_input":"2025-09-10T04:31:28.353981Z","iopub.status.idle":"2025-09-10T04:31:28.501907Z","shell.execute_reply.started":"2025-09-10T04:31:28.353953Z","shell.execute_reply":"2025-09-10T04:31:28.501315Z"}},"outputs":[{"name":"stdout","text":"Fetching daily bulk deal data...\n✅ Bulk deal data fetched.\nParsing chat log for messages by 'NEO APEX VENTURE LLP'...\n✅ Found 0 stock mentions by the advisor.\n\n================================================================================\n          Cross-Correlation Analysis Report\n================================================================================\nNo stock mentions found for 'NEO APEX VENTURE LLP' in the chat log.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 1: INSTALLATIONS & IMPORTS\n# ==============================================================================\n\n# Install required packages\n!pip install pandas scikit-learn joblib opencv-python easyocr python-whois requests beautifulsoup4 html5lib yfinance -q\n\n# --- Standard Library Imports ---\nimport os\nimport re\nimport glob\nimport json\nimport datetime\nfrom collections import Counter, defaultdict\nfrom urllib.parse import urlparse\nfrom io import StringIO\n\n# --- Suppress Warnings ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Core Data Science & ML Imports ---\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# --- Image & Web Scraping Imports ---\nimport cv2\nimport easyocr\nimport whois\nimport requests\nimport yfinance as yf\n\nprint(\"✅ All libraries loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:31:28.503362Z","iopub.execute_input":"2025-09-10T04:31:28.503801Z","iopub.status.idle":"2025-09-10T04:33:06.332271Z","shell.execute_reply.started":"2025-09-10T04:31:28.503783Z","shell.execute_reply":"2025-09-10T04:33:06.331588Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h✅ All libraries loaded successfully.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 2: BASELINE MODEL TRAINING\n# ==============================================================================\n\ndef train_scam_classifier():\n    \"\"\"\n    Loads the dataset, trains a text classifier, and saves the model\n    and vectorizer to the /kaggle/working/ directory.\n    \"\"\"\n    print(\"🚀 Starting model training process...\")\n    try:\n        # Define paths for the writable Kaggle directory\n        WORKING_DIR = \"/kaggle/working/\"\n        VECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\n        MODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n\n        # Load dataset\n        df = pd.read_csv(\"/kaggle/input/whatsapp-scam/whatsapp_scam_dataset.csv\")\n\n        # Define features & labels\n        X = df[\"message\"]\n        y = df[\"scam_type\"]\n\n        # Train-test split\n        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n        # Text vectorization\n        vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000, ngram_range=(1,2))\n        X_train_vec = vectorizer.fit_transform(X_train)\n\n        # Train model\n        model = LogisticRegression(max_iter=200, class_weight=\"balanced\")\n        model.fit(X_train_vec, y_train)\n\n        # Save the model and vectorizer\n        joblib.dump(vectorizer, VECTORIZER_PATH)\n        joblib.dump(model, MODEL_PATH)\n        \n        print(f\"✅ Model and vectorizer saved successfully to {WORKING_DIR}\")\n        return True\n\n    except Exception as e:\n        print(f\"❌ An error occurred during model training: {e}\")\n        print(\"    Ensure '/kaggle/input/whatsapp-scam/whatsapp_scam_dataset.csv' is added to your notebook.\")\n        return False\n\n# Run the training\ntrain_scam_classifier()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:33:06.333077Z","iopub.execute_input":"2025-09-10T04:33:06.333471Z","iopub.status.idle":"2025-09-10T04:33:07.830798Z","shell.execute_reply.started":"2025-09-10T04:33:06.333447Z","shell.execute_reply":"2025-09-10T04:33:07.830076Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting model training process...\n✅ Model and vectorizer saved successfully to /kaggle/working/\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 3: COMPILED ANALYSIS PIPELINE\n# ==============================================================================\n\n# --- Helper Function: URL Intelligence ---\ndef check_url_risk(url):\n    score, reasons = 0, []\n    try:\n        domain = urlparse(url).netloc\n        if not url.startswith(\"https\"): score += 30; reasons.append(\"URL is not secure (HTTP)\")\n        if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain): score += 40; reasons.append(\"URL uses an IP address\")\n        if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]): score += 25; reasons.append(\"URL uses a suspicious TLD\")\n        try:\n            domain_info = whois.whois(domain)\n            creation_date = domain_info.creation_date\n            if creation_date:\n                creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                age_days = (datetime.datetime.now() - creation_date).days\n                if age_days < 180: score += 30; reasons.append(f\"Domain is very new ({age_days} days old)\")\n        except Exception: score += 10; reasons.append(\"WHOIS lookup failed\")\n    except Exception: return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL is malformed\"]}\n    risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n    return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n# --- Helper Function: Image & QR Intelligence ---\ndef check_qr_code(image_path):\n    try:\n        img = cv2.imread(image_path)\n        if img is None: return None\n        qr_detector = cv2.QRCodeDetector()\n        data, _, _ = qr_detector.detectAndDecode(img)\n        if data: return {\"qr_data\": data, \"analysis\": check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]}}\n    except Exception: return None\n    return None\n\ndef analyze_image_content(image_path, reader, text_model, text_vectorizer):\n    results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": check_qr_code(image_path)}\n    try:\n        extracted_text = \" \".join(reader.readtext(image_path, detail=0, paragraph=True))\n        if extracted_text.strip():\n            results[\"ocr_text\"] = extracted_text\n            vec = text_vectorizer.transform([extracted_text])\n            results[\"text_scam_prediction\"] = text_model.predict(vec)[0]\n    except Exception: pass\n    return results\n\n# --- Helper Function: SEBI & Stock Mention (Placeholders) ---\ndef verify_sebi_id(sebi_id):\n    if not sebi_id or not sebi_id.strip(): return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n    print(f\"⚠️  Running placeholder for SEBI ID: {sebi_id}\")\n    return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\ndef analyze_stock_mentions(text):\n    stock_symbols = re.findall(r'\\b[A-Z]{3,}\\b', text)\n    if not stock_symbols: return None\n    suspicious_patterns = [\"Message contains pump-and-dump keywords.\"] if any(kw in text.lower() for kw in [\"guaranteed\", \"10x\", \"insider\"]) else []\n    return {\"mentioned_stocks\": list(set(stock_symbols)), \"suspicious_patterns\": suspicious_patterns}\n\n# --- Helper Function: Chat Analysis ---\ndef parse_chat_for_advisor_mentions(file_path, advisor_name):\n    stock_mentions, stock_pattern = [], re.compile(r'\\b[A-Z]{3,}\\b')\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            match = re.match(r'(\\d{2}/\\d{2}/\\d{2,4}),\\s\\d{2}:\\d{2}\\s-\\s(.*?):\\s(.*)', line)\n            if match and advisor_name.lower() in match.group(2).lower():\n                date_str, _, message = match.groups()\n                try: message_date = datetime.datetime.strptime(date_str, '%d/%m/%y')\n                except ValueError: message_date = datetime.datetime.strptime(date_str, '%d/%m/%Y')\n                for symbol in stock_pattern.findall(message):\n                    stock_mentions.append({\"date\": message_date, \"stock_symbol\": symbol, \"message\": message.strip()})\n    return stock_mentions\n\ndef detect_bot_behavior(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f: lines = f.readlines()\n    users = [re.match(r'.*?-\\s(.*?):', line).group(1).strip() for line in lines if re.match(r'.*?-\\s(.*?):', line)]\n    if not users: return {\"error\": \"No users found.\"}\n    user_counts = Counter(users)\n    admin_name = user_counts.most_common(1)[0][0]\n    return {\"detected_admin\": admin_name, \"message_counts\": dict(user_counts)}\n\n# --- Helper Function: Bulk Deal & Cross-Correlation ---\ndef get_bse_bulk_deals():\n    try:\n        url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n        headers = {'User-Agent': 'Mozilla/5.0'}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        for table in pd.read_html(StringIO(response.text)):\n            if 'Client Name' in table.columns:\n                table.columns = table.columns.str.strip()\n                table['Deal Date'] = pd.to_datetime(table['Deal Date'], format='%d/%m/%Y')\n                return table\n    except Exception: return pd.DataFrame()\n    return pd.DataFrame()\n\ndef cross_correlation_engine(chat_file, advisor_name, trades_df):\n    if trades_df.empty or not advisor_name: return [\"Bulk deal data not available or no advisor name provided.\"]\n    advisor_trades = trades_df[trades_df['Client Name'].str.contains(advisor_name, case=False, na=False)]\n    if advisor_trades.empty: return [f\"No bulk deals found for '{advisor_name}'.\"]\n    advisor_mentions = parse_chat_for_advisor_mentions(chat_file, advisor_name)\n    if not advisor_mentions: return [f\"No stock mentions by '{advisor_name}' found in chat.\"]\n    red_flags = []\n    for mention in advisor_mentions:\n        for _, trade in advisor_trades.iterrows():\n            if mention['stock_symbol'].lower() in trade['Security Name'].lower():\n                time_delta = mention['date'] - trade['Deal Date']\n                if trade['Deal Type'].lower() == 'buy' and 0 <= time_delta.days <= 7:\n                    red_flags.append(f\"🚨 POTENTIAL FRONT-RUNNING: Advisor promoted '{mention['stock_symbol']}' on {mention['date'].date()} just {time_delta.days} day(s) AFTER a bulk BUY.\")\n                if trade['Deal Type'].lower() == 'sell' and 0 <= time_delta.days <= 30:\n                    red_flags.append(f\"🚨 POTENTIAL PUMP & DUMP: Advisor made a bulk SELL of {trade['Security Name']} {time_delta.days} day(s) AFTER promoting '{mention['stock_symbol']}'.\")\n    return red_flags if red_flags else [\"No suspicious correlations found.\"]\n\n# --- THE MAIN PIPELINE FUNCTION ---\ndef run_analysis_pipeline(chat_file_path, image_folder_path, sebi_id_to_check, entity_name_to_track):\n    \"\"\"Orchestrates the entire fraud detection and analysis process.\"\"\"\n    print(\"🚀 Starting Full Analysis Pipeline...\")\n    WORKING_DIR = \"/kaggle/working/\"\n    VECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\n    MODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n    try:\n        vectorizer = joblib.load(VECTORIZER_PATH)\n        model = joblib.load(MODEL_PATH)\n        reader = easyocr.Reader(['en'], gpu=False) # Set gpu=False for Kaggle CPU environment\n    except Exception as e: return {\"error\": f\"Failed to load models or OCR: {e}\"}\n\n    final_report = defaultdict(dict)\n    \n    # 1. Perform one-time analyses\n    print(\"📈 Analyzing market and chat-level data...\")\n    final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = verify_sebi_id(sebi_id_to_check)\n    bulk_deals_df = get_bse_bulk_deals()\n    final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n    final_report[\"cross_correlation_analysis\"] = cross_correlation_engine(chat_file_path, entity_name_to_track, bulk_deals_df)\n    final_report[\"chat_analysis\"] = detect_bot_behavior(chat_file_path)\n\n    # 2. Perform message-by-message and image analysis\n    print(\"🔬 Analyzing individual messages and images...\")\n    messages = parse_chat_for_advisor_mentions(chat_file_path, \"\") # Get all messages\n    scam_predictions = []\n    final_report[\"message_by_message_analysis\"] = []\n    for msg_data in messages:\n        text = msg_data[\"message\"]\n        analysis = {\"text_scam_prediction\": model.predict(vectorizer.transform([text]))[0]}\n        scam_predictions.append(analysis[\"text_scam_prediction\"])\n        if re.search(r'(https?://\\S+)', text):\n            analysis[\"url_analysis\"] = [check_url_risk(url) for url in re.findall(r'(https?://\\S+)', text)]\n        if analyze_stock_mentions(text):\n            analysis[\"stock_mention_analysis\"] = analyze_stock_mentions(text)\n        if len(analysis) > 1:\n            final_report[\"message_by_message_analysis\"].append({\"message\": text, \"analysis\": analysis})\n    \n    image_files = glob.glob(os.path.join(image_folder_path, \"*[.png|.jpg|.jpeg]\"))\n    final_report[\"image_analysis\"] = []\n    for img_path in image_files:\n        final_report[\"image_analysis\"].append({\"file_name\": os.path.basename(img_path), \"analysis\": analyze_image_content(img_path, reader, model, vectorizer)})\n\n    # 3. Compile summary\n    final_report[\"summary\"] = {\n        \"total_messages_analyzed\": len(messages),\n        \"total_images_analyzed\": len(image_files),\n        \"scam_types_detected\": dict(Counter(p for p in scam_predictions if p != \"not_scam\"))\n    }\n    print(\"\\n✅ Pipeline finished successfully!\")\n    return final_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:33:07.831924Z","iopub.execute_input":"2025-09-10T04:33:07.832187Z","iopub.status.idle":"2025-09-10T04:33:07.854319Z","shell.execute_reply.started":"2025-09-10T04:33:07.832167Z","shell.execute_reply":"2025-09-10T04:33:07.853775Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 4: CONFIGURE AND RUN\n# ==============================================================================\n\n# --- 🔽 IMPORTANT: SET YOUR FILE PATHS AND INPUTS HERE 🔽 ---\n\n# 1. Path to your uploaded chat TXT file\nimport os\nchat_file = \"/kaggle/input/whatsap-chats-downloaded/WhatsApp Chat with TriDevs.txt\"\n\n# 2. Path to your folder containing the images\nimage_folder = \"/kaggle/input/scam-qr-png/\"\n\n# 3. The SEBI Registration ID you want to verify (can be left blank)\nsebi_id_input = \"INZ000048660\"\n\n# 4. The registered NAME of the advisor to track in chats and bulk deals\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\n# --- 🔼 NO MORE CHANGES NEEDED BELOW THIS LINE 🔼 ---\n\n# Run the main pipeline\nif os.path.exists(chat_file):\n    results = run_analysis_pipeline(\n        chat_file_path=chat_file,\n        image_folder_path=image_folder,\n        sebi_id_to_check=sebi_id_input,\n        entity_name_to_track=entity_name_input\n    )\n    \n    # Print the final report as a clean JSON\n    print(\"\\n\" + \"=\"*50)\n    print(\"          FINAL FRAUD DETECTION REPORT\")\n    print(\"=\"*50 + \"\\n\")\n    print(json.dumps(results, indent=4))\nelse:\n    print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import all necessary libraries at the top\nimport os\nimport re\nimport json\nimport joblib\nimport whois\nimport cv2\nimport easyocr\nimport requests\nimport datetime\nimport pandas as pd\nfrom io import StringIO\nfrom urllib.parse import urlparse\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport glob\n\n# ==============================================================================\n# SECTION 1: THE PIPELINE CLASS DEFINITION\n# ==============================================================================\n\nclass FraudDetectionPipeline:\n    \"\"\"\n    A self-contained class to run the multi-modal scam detection pipeline.\n    \n    This class loads all necessary models and provides a single 'run' method\n    to perform the analysis on the input files.\n    \"\"\"\n    def __init__(self, model_path, vectorizer_path):\n        \"\"\"\n        Initializes the pipeline by loading the ML model, vectorizer, and OCR reader.\n        \"\"\"\n        print(\"🚀 Initializing Fraud Detection Pipeline...\")\n        try:\n            self.model = joblib.load(model_path)\n            self.vectorizer = joblib.load(vectorizer_path)\n            # Initialize OCR reader (it will download models on first run)\n            self.ocr_reader = easyocr.Reader(['en'], gpu=False) \n            print(\"✅ Pipeline initialized successfully.\")\n        except Exception as e:\n            raise IOError(f\"❌ Failed to load models. Ensure files exist at the specified paths. Error: {e}\")\n\n    # --- All your helper functions go here as methods of the class ---\n    # Note that we add 'self' as the first argument to each function.\n\n    def _check_url_risk(self, url):\n        score, reasons = 0, []\n        try:\n            domain = urlparse(url).netloc\n            if not url.startswith(\"https\"): score += 30; reasons.append(\"URL is not secure (HTTP)\")\n            if re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain): score += 40; reasons.append(\"URL uses an IP address\")\n            if any(domain.endswith(tld) for tld in [\".xyz\", \".top\", \".biz\", \".shop\"]): score += 25; reasons.append(\"URL uses a suspicious TLD\")\n            try:\n                domain_info = whois.whois(domain)\n                creation_date = domain_info.creation_date\n                if creation_date:\n                    creation_date = creation_date[0] if isinstance(creation_date, list) else creation_date\n                    age_days = (datetime.datetime.now() - creation_date).days\n                    if age_days < 180: score += 30; reasons.append(f\"Domain is very new ({age_days} days old)\")\n            except Exception: score += 10; reasons.append(\"WHOIS lookup failed\")\n        except Exception: return {\"url\": url, \"risk_level\": \"High\", \"risk_score\": 100, \"reasons\": [\"URL is malformed\"]}\n        risk_level = \"High\" if score >= 60 else \"Medium\" if score >= 30 else \"Low\"\n        return {\"url\": url, \"risk_level\": risk_level, \"risk_score\": min(score, 100), \"reasons\": reasons}\n\n    def _check_qr_code(self, image_path):\n        try:\n            img = cv2.imread(image_path)\n            if img is None: return None\n            qr_detector = cv2.QRCodeDetector()\n            data, _, _ = qr_detector.detectAndDecode(img)\n            if data: return {\"qr_data\": data, \"analysis\": self._check_url_risk(data) if data.startswith(\"http\") else {\"risk_level\": \"Low\", \"reasons\": [\"QR contains non-URL text\"]}}\n        except Exception: return None\n        return None\n        \n    def _analyze_image_content(self, image_path):\n        results = {\"ocr_text\": \"\", \"text_scam_prediction\": \"N/A\", \"qr_analysis\": self._check_qr_code(image_path)}\n        try:\n            extracted_text = \" \".join(self.ocr_reader.readtext(image_path, detail=0, paragraph=True))\n            if extracted_text.strip():\n                results[\"ocr_text\"] = extracted_text\n                vec = self.vectorizer.transform([extracted_text])\n                results[\"text_scam_prediction\"] = self.model.predict(vec)[0]\n        except Exception: pass\n        return results\n\n    def _verify_sebi_id(self, sebi_id):\n        if not sebi_id or not sebi_id.strip(): return {\"sebi_id\": \"Not Provided\", \"status\": \"N/A\"}\n        print(f\"⚠️  Running placeholder for SEBI ID: {sebi_id}\")\n        return {\"sebi_id\": sebi_id, \"status\": \"Verified (Dummy)\" if len(sebi_id) > 10 and \"INA\" in sebi_id else \"Not Found (Dummy)\"}\n\n    def _analyze_stock_mentions(self, text):\n        stock_symbols = re.findall(r'\\b[A-Z]{3,}\\b', text)\n        if not stock_symbols: return None\n        suspicious_patterns = [\"Message contains pump-and-dump keywords.\"] if any(kw in text.lower() for kw in [\"guaranteed\", \"10x\", \"insider\"]) else []\n        return {\"mentioned_stocks\": list(set(stock_symbols)), \"suspicious_patterns\": suspicious_patterns}\n\n    def _parse_chat_for_messages(self, file_path, user_filter=\"\"):\n        messages = []\n        stock_pattern = re.compile(r'\\b[A-Z]{3,}\\b')\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                match = re.match(r'(\\d{2}/\\d{2}/\\d{2,4}),\\s\\d{2}:\\d{2}\\s-\\s(.*?):\\s(.*)', line)\n                if match:\n                    date_str, user, message = match.groups()\n                    if user_filter.lower() in user.lower():\n                        try: message_date = datetime.datetime.strptime(date_str, '%d/%m/%y')\n                        except ValueError: message_date = datetime.datetime.strptime(date_str, '%d/%m/%Y')\n                        messages.append({\n                            \"date\": message_date,\n                            \"user\": user,\n                            \"stock_symbols\": stock_pattern.findall(message),\n                            \"message\": message.strip()\n                        })\n        return messages\n\n    def _detect_bot_behavior(self, file_path):\n        with open(file_path, 'r', encoding='utf-8') as f: lines = f.readlines()\n        users = [re.match(r'.*?-\\s(.*?):', line).group(1).strip() for line in lines if re.match(r'.*?-\\s(.*?):', line)]\n        if not users: return {\"error\": \"No users found.\"}\n        user_counts = Counter(users)\n        admin_name = user_counts.most_common(1)[0][0]\n        return {\"detected_admin\": admin_name, \"message_counts\": dict(user_counts)}\n\n    def _get_bse_bulk_deals(self):\n        try:\n            url = \"https://www.bseindia.com/markets/equity/EQReports/bulk_deals.aspx\"\n            headers = {'User-Agent': 'Mozilla/5.0'}\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            for table in pd.read_html(StringIO(response.text)):\n                if 'Client Name' in table.columns:\n                    table.columns = table.columns.str.strip()\n                    table['Deal Date'] = pd.to_datetime(table['Deal Date'], format='%d/%m/%Y')\n                    return table\n        except Exception: return pd.DataFrame()\n        return pd.DataFrame()\n\n    def _cross_correlation_engine(self, chat_file, advisor_name, trades_df):\n        if trades_df.empty or not advisor_name: return [\"Bulk deal data not available or no advisor name provided.\"]\n        advisor_trades = trades_df[trades_df['Client Name'].str.contains(advisor_name, case=False, na=False)]\n        if advisor_trades.empty: return [f\"No bulk deals found for '{advisor_name}'.\"]\n        \n        advisor_mentions = self._parse_chat_for_messages(chat_file, user_filter=advisor_name)\n        if not advisor_mentions: return [f\"No stock mentions by '{advisor_name}' found in chat.\"]\n        \n        red_flags = []\n        for mention in advisor_mentions:\n            for symbol in mention['stock_symbols']:\n                for _, trade in advisor_trades.iterrows():\n                    if symbol.lower() in trade['Security Name'].lower():\n                        time_delta = mention['date'] - trade['Deal Date']\n                        if trade['Deal Type'].lower() == 'buy' and 0 <= time_delta.days <= 7:\n                            red_flags.append(f\"🚨 POTENTIAL FRONT-RUNNING: Advisor promoted '{symbol}' on {mention['date'].date()} just {time_delta.days} day(s) AFTER a bulk BUY.\")\n                        if trade['Deal Type'].lower() == 'sell' and 0 <= time_delta.days <= 30:\n                            red_flags.append(f\"🚨 POTENTIAL PUMP & DUMP: Advisor made a bulk SELL of {trade['Security Name']} {time_delta.days} day(s) AFTER promoting '{symbol}'.\")\n        return red_flags if red_flags else [\"No suspicious correlations found.\"]\n    \n    # --- THE MAIN 'RUN' METHOD ---\n    def run(self, chat_file_path, image_folder_path, sebi_id_to_check, entity_name_to_track):\n        \"\"\"\n        Orchestrates the entire fraud detection and analysis process.\n        \"\"\"\n        print(\"🚀 Starting Full Analysis Pipeline...\")\n        final_report = defaultdict(dict)\n        \n        # 1. Perform one-time analyses\n        print(\"📈 Analyzing market and chat-level data...\")\n        final_report[\"manual_verifications\"][\"sebi_id_analysis\"] = self._verify_sebi_id(sebi_id_to_check)\n        bulk_deals_df = self._get_bse_bulk_deals()\n        final_report[\"bulk_deal_analysis\"] = {\"trades_found_today\": len(bulk_deals_df)}\n        final_report[\"cross_correlation_analysis\"] = self._cross_correlation_engine(chat_file_path, entity_name_to_track, bulk_deals_df)\n        final_report[\"chat_analysis\"] = self._detect_bot_behavior(chat_file_path)\n\n        # 2. Perform message-by-message and image analysis\n        print(\"🔬 Analyzing individual messages and images...\")\n        all_messages = self._parse_chat_for_messages(chat_file_path) # Get all messages\n        scam_predictions = []\n        final_report[\"message_by_message_analysis\"] = []\n        for msg_data in all_messages:\n            text = msg_data[\"message\"]\n            analysis = {\"text_scam_prediction\": self.model.predict(self.vectorizer.transform([text]))[0]}\n            scam_predictions.append(analysis[\"text_scam_prediction\"])\n            \n            if re.search(r'(https?://\\S+)', text):\n                analysis[\"url_analysis\"] = [self._check_url_risk(url) for url in re.findall(r'(https?://\\S+)', text)]\n            \n            stock_analysis = self._analyze_stock_mentions(text)\n            if stock_analysis:\n                analysis[\"stock_mention_analysis\"] = stock_analysis\n\n            if len(analysis) > 1: # Only add if there's more than just the scam prediction\n                final_report[\"message_by_message_analysis\"].append({\"message\": text, \"analysis\": analysis})\n        \n        image_files = glob.glob(os.path.join(image_folder_path, \"*[.png|.jpg|.jpeg]\"))\n        final_report[\"image_analysis\"] = []\n        for img_path in image_files:\n            final_report[\"image_analysis\"].append({\n                \"file_name\": os.path.basename(img_path),\n                \"analysis\": self._analyze_image_content(img_path)\n            })\n\n        # 3. Compile summary\n        final_report[\"summary\"] = {\n            \"total_messages_analyzed\": len(all_messages),\n            \"total_images_analyzed\": len(image_files),\n            \"scam_types_detected\": dict(Counter(p for p in scam_predictions if p != \"not_scam\"))\n        }\n        print(\"\\n✅ Pipeline finished successfully!\")\n        return final_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:43:57.234080Z","iopub.execute_input":"2025-09-10T04:43:57.234328Z","iopub.status.idle":"2025-09-10T04:43:57.261666Z","shell.execute_reply.started":"2025-09-10T04:43:57.234313Z","shell.execute_reply":"2025-09-10T04:43:57.261077Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 2: CONFIGURE AND RUN THE PIPELINE\n# ==============================================================================\n\n# First, ensure your baseline model is trained and saved.\n# You only need to run this function once to create the .pkl files.\n# train_scam_classifier() # Assuming this was already run and files are saved.\n\n# --- 🔽 SET YOUR FILE PATHS AND INPUTS HERE 🔽 ---\nWORKING_DIR = \"/kaggle/working/\"\nVECTORIZER_PATH = os.path.join(WORKING_DIR, \"vectorizer.pkl\")\nMODEL_PATH = os.path.join(WORKING_DIR, \"scam_model.pkl\")\n\n# Input data paths\nchat_file = \"/kaggle/input/whatsap-chats-downloaded/WhatsApp Chat with TriDevs.txt\"\nimage_folder = \"/kaggle/input/scam-qr-png/\"\n\n# Entity information\nsebi_id_input = \"INZ000048660\"\nentity_name_input = \"NEO APEX VENTURE LLP\"\n\n# --- 🔼 NO MORE CHANGES NEEDED BELOW THIS LINE 🔼 ---\n\ntry:\n    # 1. Create an instance of the pipeline. This loads all models.\n    fraud_detector = FraudDetectionPipeline(model_path=MODEL_PATH, vectorizer_path=VECTORIZER_PATH)\n    \n    # 2. Run the full analysis with one simple command.\n    if os.path.exists(chat_file):\n        results = fraud_detector.run(\n            chat_file_path=chat_file,\n            image_folder_path=image_folder,\n            sebi_id_to_check=sebi_id_input,\n            entity_name_to_track=entity_name_input\n        )\n        \n        # 3. Print the final report.\n        print(\"\\n\" + \"=\"*50)\n        print(\"                 FINAL FRAUD DETECTION REPORT\")\n        print(\"=\"*50 + \"\\n\")\n        print(json.dumps(results, indent=4, default=str)) # Use default=str to handle datetime objects\n    else:\n        print(f\"❌ ERROR: The chat file was not found at the path: {chat_file}\")\n\nexcept IOError as e:\n    print(e)\n    print(\"Please run the 'train_scam_classifier()' function first to generate model files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}